{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not make prediction. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Use ThreadPoolExecutor to process messages concurrently\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Apply the translation function to each row and store the result in a new column\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     df_100[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslated_messages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(executor\u001b[38;5;241m.\u001b[39mmap(process_message, [row \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df_100\u001b[38;5;241m.\u001b[39miterrows()]))\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Save the updated DataFrame with translated messages to a new CSV file\u001b[39;00m\n\u001b[1;32m     68\u001b[0m df_100\u001b[38;5;241m.\u001b[39mto_csv(output_file, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop())\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fut\u001b[38;5;241m.\u001b[39mresult(timeout)\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[0;32mIn[1], line 52\u001b[0m, in \u001b[0;36mprocess_message\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     43\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     44\u001b[0m     system_message,\n\u001b[1;32m     45\u001b[0m     {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     }\n\u001b[1;32m     49\u001b[0m ]\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Send the message to the PredictionGuard API\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m result \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     53\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHermes-3-Llama-3.1-8B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     54\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Extract the chatbot's response\u001b[39;00m\n\u001b[1;32m     58\u001b[0m response \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/predictionguard/client.py:194\u001b[0m, in \u001b[0;36mChatCompletions.create\u001b[0;34m(self, model, messages, input, output, max_tokens, temperature, top_p, top_k, stream)\u001b[0m\n\u001b[1;32m    181\u001b[0m args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    182\u001b[0m     model,\n\u001b[1;32m    183\u001b[0m     messages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m     stream,\n\u001b[1;32m    191\u001b[0m )\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Run _generate_chat\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m choices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_chat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m choices\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/predictionguard/client.py:350\u001b[0m, in \u001b[0;36mChatCompletions._generate_chat\u001b[0;34m(self, model, messages, input, output, max_tokens, temperature, top_p, top_k, stream)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stream_generator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl, headers, payload, stream)\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl, headers, payload)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/predictionguard/client.py:230\u001b[0m, in \u001b[0;36mChatCompletions._generate_chat.<locals>.return_dict\u001b[0;34m(url, headers, payload)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not make prediction. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m err)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not make prediction. "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from predictionguard import PredictionGuard\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "\n",
    "# Set your API key\n",
    "api_key = os.getenv(\"PREDICTIONGUARD_API_KEY\", \"Oq62vYfSJRwjnFQcUnJy5PM3SRVejYtJCXWSxnfv\")\n",
    "\n",
    "# Initialize the PredictionGuard client\n",
    "client = PredictionGuard(api_key=api_key)\n",
    "\n",
    "# System content for the chatbot behavior based on your refined prompt\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": (\n",
    "        \"You are a language identification and translation system. Your task is to process a message and return it in the following way:\\n\"\n",
    "        \"1. If the message is entirely in English, return the message exactly as is.\\n\"\n",
    "        \"2. If the message is in both English and another language:\\n\"\n",
    "        \"    - If the foreign language part is a direct translation of the English text, return only the English.\\n\"\n",
    "        \"    - If the foreign language contains information that is not a direct translation, translate the entire message to English and return it.\\n\"\n",
    "        \"3. If the message is entirely in a foreign language, translate it into English and return it.\\n\"\n",
    "        \"4. If a message seems to be unrelated to reviews, is spam, or contains malicious content, output SPAM\\n\"\n",
    "        \"Do not provide any explanations or contextual information. Only return the final processed message.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Input and output CSV file paths\n",
    "csv_file = 'beer_reviews.csv'\n",
    "output_file = 'translated_messages.csv'\n",
    "\n",
    "# Read the CSV file using pandas\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Group the DataFrame by the beer name (first column) and take the first 100 reviews for each beer\n",
    "df_100 = df.groupby(df.columns[0]).head(100)\n",
    "\n",
    "\n",
    "# Function to process each message using PredictionGuard API\n",
    "def process_message(row):\n",
    "    try:\n",
    "        user_message = row['message']  # Adjust column name as per your CSV\n",
    "        \n",
    "        # Prepare the messages list for the chatbot\n",
    "        messages = [\n",
    "            system_message,\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Do not provide any explanations or contextual information. Only return the final processed message for : {user_message}\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Send the message to the PredictionGuard API\n",
    "        result = client.chat.completions.create(\n",
    "            model=\"Hermes-3-Llama-3.1-8B\",\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        # Extract the chatbot's response\n",
    "        response = result['choices'][0]['message']['content']\n",
    "        time.sleep(0.2)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Use ThreadPoolExecutor to process messages concurrently\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Apply the translation function to each row and store the result in a new column\n",
    "    df_100['translated_messages'] = list(executor.map(process_message, [row for _, row in df_100.iterrows()]))\n",
    "\n",
    "# Save the updated DataFrame with translated messages to a new CSV file\n",
    "df_100.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Translated messages have been saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
