{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Crowdsourced Recommender System\n",
    "\n",
    "### Group Members: Jose Currea, Jenna Ferguson, Evan Hadd, Ramzi Kattan, Hadley Krummel, Jennifer Gonzales, Ibrahim Muhammad\n",
    "### Class Section: Afternoon 1 - 3pm\n",
    "\n",
    "It should accept user inputs in the form of desired attributes of a product and come up with 3 recommendations. \n",
    "\n",
    "**Your Python Notebook should include the following:**\n",
    "- All scripts \n",
    "- The sentiment and similarity scores for the three products you recommended in task E.\n",
    "- Your analyses for and answer to task F. Make sure you show the ratings, similarity scores and sentiments for the products you recommend in tasks E and F. Use tables whenever possible.  \n",
    "- Show the logic you are using in addition to finding the most similar product. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.expand_frame_repr', False)  # Prevent wrapping to multiple lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A\n",
    "\n",
    "Extract about 5-6k reviews. However, many reviews may not have any text and will therefore be discarded. Finally you may end up with 1700-2000 reviews with text.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up ChromeDriver path\n",
    "driver_path = \"/Users/ramzikattan/Downloads/chromedriver-mac-arm64/chromedriver\"\n",
    "chrome_path = \"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\"\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.binary_location = chrome_path\n",
    "\n",
    "# Set up the Chrome WebDriver service\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the webpage\n",
    "url = \"https://www.ratebeer.com/top-beers?time=all\"\n",
    "driver.get(url)\n",
    "time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "# Parse the page with BeautifulSoup\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Close the browser after fetching the page\n",
    "driver.quit()\n",
    "\n",
    "# Find all <a> tags with the class containing the beer name and link\n",
    "beer_links = soup.find_all('a', class_=\"MuiTypography-root\")\n",
    "\n",
    "# Extract the name and URL for each beer\n",
    "beers = []\n",
    "for beer in beer_links:\n",
    "    name = beer.get_text(strip=True)  # Get the text (beer name)\n",
    "    link = beer['href']  # Get the URL\n",
    "    full_link = \"https://www.ratebeer.com\" + link  # Construct full URL\n",
    "    beers.append({'name': name, 'link': full_link})\n",
    "\n",
    "# Print the extracted beer names and their URLs\n",
    "#for beer in beers:\n",
    " #   print(f\"Beer: {beer['name']} - URL: {beer['link']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the matrix for only the beer links\n",
    "cleaned_beers = [beer for beer in beers[28:128] if beer['name']]  # Only keep non-empty names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beer Name</th>\n",
       "      <th>Beer Rating</th>\n",
       "      <th>Beer URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aecht Schlenkerla Rauchbier Urbock</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://www.ratebeer.com/beer/aecht-schlenkerl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Duckpond Duckpond Darkwing De Luxe</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://www.ratebeer.com/beer/duckpond-duckpon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aecht Schlenkerla Weichsel Rotbier</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://www.ratebeer.com/beer/aecht-schlenkerl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ayinger Altbairisch Dunkel</td>\n",
       "      <td>3.6</td>\n",
       "      <td>https://www.ratebeer.com/beer/ayinger-altbairi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Russian River Beatification (Batch 002+)</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://www.ratebeer.com/beer/russian-river-be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Beer Name  Beer Rating  \\\n",
       "0        Aecht Schlenkerla Rauchbier Urbock          4.0   \n",
       "1        Duckpond Duckpond Darkwing De Luxe          3.8   \n",
       "2        Aecht Schlenkerla Weichsel Rotbier          3.8   \n",
       "3                Ayinger Altbairisch Dunkel          3.6   \n",
       "4  Russian River Beatification (Batch 002+)          4.2   \n",
       "\n",
       "                                            Beer URL  \n",
       "0  https://www.ratebeer.com/beer/aecht-schlenkerl...  \n",
       "1  https://www.ratebeer.com/beer/duckpond-duckpon...  \n",
       "2  https://www.ratebeer.com/beer/aecht-schlenkerl...  \n",
       "3  https://www.ratebeer.com/beer/ayinger-altbairi...  \n",
       "4  https://www.ratebeer.com/beer/russian-river-be...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rescrape Using Nico's Links\n",
    "cleaned_beers = pd.read_csv('beer_url.csv')\n",
    "\n",
    "# Drop rows with duplicate beer names, keeping the first occurrence\n",
    "cleaned_beers = cleaned_beers.drop_duplicates(subset=['Beer Name'], keep='first')\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "cleaned_beers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Chrome options for headless mode\n",
    "chrome_options = Options()\n",
    "chrome_options.binary_location = chrome_path  # Path to your Chrome browser\n",
    "chrome_options.add_argument('--headless')  # Enable headless mode to run faster\n",
    "chrome_options.add_argument('--disable-gpu')  # Disable GPU acceleration (for better performance in headless mode)\n",
    "chrome_options.add_argument('--no-sandbox')  # Added for safe execution in certain environments\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')  # Avoid issues with shared memory\n",
    "\n",
    "# Set up the Chrome WebDriver service\n",
    "service = Service(driver_path)  # Path to your ChromeDriver\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Function to click \"Show More\" buttons on the current page\n",
    "def click_show_more_buttons():\n",
    "    # Locate all \"Show More\" buttons on the page\n",
    "    show_more_xpath = '//span[contains(@class, \"MuiButton-label\") and text()=\"Show more\"]'\n",
    "    show_more_buttons = driver.find_elements(By.XPATH, show_more_xpath)\n",
    "\n",
    "    # Click each \"Show More\" button\n",
    "    for button in show_more_buttons:\n",
    "        try:\n",
    "            if button.is_displayed():\n",
    "                driver.execute_script(\"arguments[0].click();\", button)\n",
    "                time.sleep(1)  # Wait for the content to expand\n",
    "        except:\n",
    "            pass  # Skip if there's an issue clicking the button\n",
    "\n",
    "# Function to scrape reviews on the current page\n",
    "def scrape_reviews():\n",
    "    reviews = []\n",
    "    ratings_xpath = '//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"bRPQdN\", \" \" )) and contains(concat( \" \", @class, \" \" ), concat( \" \", \"MuiTypography-subtitle1\", \" \" ))]'  # Adjust this based on your page's structure for ratings\n",
    "    messages_xpath = '//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"pre-wrap\", \" \" )) and contains(concat( \" \", @class, \" \" ), concat( \" \", \"MuiTypography-body1\", \" \" ))]'  # Adjust this based on your page's structure for review messages\n",
    "\n",
    "    # Find all the review ratings and messages on the current page\n",
    "    ratings = driver.find_elements(By.XPATH, ratings_xpath)\n",
    "    messages = driver.find_elements(By.XPATH, messages_xpath)\n",
    "\n",
    "    for i, message in enumerate(messages):\n",
    "        reviews.append({\n",
    "            'rating': ratings[i].text if i < len(ratings) else None,  # Handle index if ratings and messages mismatch\n",
    "            'message': message.text\n",
    "        })\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Function to scrape reviews for a single beer URL with a limit of 250 reviews\n",
    "def scrape_beer_reviews(beer_name, url, review_limit):\n",
    "    all_reviews = []\n",
    "    \n",
    "    driver.get(url)\n",
    "\n",
    "    # Handle the cookies banner by accepting it\n",
    "    try:\n",
    "        accept_cookies_id = 'onetrust-accept-btn-handler'\n",
    "        accept_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, accept_cookies_id)))\n",
    "        accept_button.click()\n",
    "    except:\n",
    "        print(\"No cookies banner found or failed to dismiss.\")\n",
    "        pass\n",
    "\n",
    "    # Loop through pages and scrape reviews until the limit is reached\n",
    "    while len(all_reviews) < review_limit:\n",
    "        # Click all \"Show More\" buttons to expand reviews\n",
    "        click_show_more_buttons()\n",
    "\n",
    "        # Scrape reviews on the current page\n",
    "        page_reviews = scrape_reviews()\n",
    "        all_reviews.extend(page_reviews)\n",
    "\n",
    "        # Check if we've hit the review limit\n",
    "        if len(all_reviews) >= review_limit:\n",
    "            all_reviews = all_reviews[:review_limit]  # Truncate to exactly the review limit\n",
    "            break\n",
    "\n",
    "        # Find the \"Next\" button and move to the next page\n",
    "        try:\n",
    "            next_button_xpath = '//button[@aria-label=\"Next page\" and contains(@class, \"MuiIconButton-root\")]'\n",
    "            next_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, next_button_xpath)))\n",
    "            next_button.click()\n",
    "\n",
    "            # Wait for the next page to load\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            # If there's no \"Next\" button, break the loop\n",
    "            print(f\"Finished scraping {beer_name}.\")\n",
    "            break\n",
    "\n",
    "    return all_reviews\n",
    "\n",
    "# Initialize an empty DataFrame to store all the reviews\n",
    "df = pd.DataFrame(columns=['Beer Name', 'URL', 'Rating', 'Review'])\n",
    "\n",
    "# Iterate through each beer in the cleaned_beers list\n",
    "for beer in cleaned_beers:\n",
    "    beer_name = beer['name']\n",
    "    beer_url = beer['link']\n",
    "\n",
    "    print(f\"Scraping reviews for {beer_name} at {beer_url}...\")\n",
    "\n",
    "    # Scrape reviews for the current beer with a limit of 250 reviews\n",
    "    reviews = scrape_beer_reviews(beer_name, beer_url, review_limit=100)\n",
    "\n",
    "    # Create a DataFrame for the reviews of the current beer\n",
    "    beer_df = pd.DataFrame(reviews)\n",
    "    beer_df['Beer Name'] = beer_name\n",
    "    beer_df['URL'] = beer_url\n",
    "\n",
    "    # Append the DataFrame for this beer to the overall DataFrame\n",
    "    df = pd.concat([df, beer_df], ignore_index=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('beer_reviews.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Close the browser after scraping all URLs\n",
    "driver.quit()\n",
    "\n",
    "print(\"Scraping completed. Data saved to 'beer_reviews.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B\n",
    "\n",
    "Assume that a customer, who will be using this recommender system, has specified 3 attributes in a product. E.g., one website describes multiple attributes of beer (but you should choose attributes from the actual data like you did for the first assignment)\n",
    "\n",
    "https://www.dummies.com/food-drink/drinks/beer/beer-for-dummies-cheat-sheet/\n",
    "- Aggressive (Boldly assertive aroma and/or taste) \n",
    "- Balanced: Malt and hops in similar proportions; equal representation of malt sweetness and hop bitterness in the flavor — especially at the finish\n",
    "- Complex: Multidimensional; many flavors and sensations on the palate\n",
    "- Crisp: Highly carbonated; effervescent\n",
    "- Fruity: Flavors reminiscent of various fruits or Hoppy: Herbal, earthy, spicy, or citric aromas and flavors of hops or Malty: Grainy, caramel-like; can be sweet or dry\n",
    "- Robust: Rich and full-bodied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from predictionguard import PredictionGuard\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input File Name\n",
    "inputfile = 'translated_messages.csv'\n",
    "\n",
    "# Translated Column Messages\n",
    "messagecolumn = 'translated_messages'\n",
    "\n",
    "# Import Data\n",
    "data = pd.read_csv(inputfile)\n",
    "data = data.drop(['Rating', 'Review'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join All Messages\n",
    "all_messages = ' '.join(data[messagecolumn].tolist())\n",
    "\n",
    "# Find All Words\n",
    "words = re.findall(r'\\b\\w+\\b', all_messages.lower())\n",
    "\n",
    "# Remove Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = [word for word in words if word not in stop_words]\n",
    "\n",
    "# Count\n",
    "counts = Counter(filtered)\n",
    "countsdf = pd.DataFrame(counts.items(), columns = ['words', 'frequency'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API key\n",
    "api_key = os.getenv(\"PREDICTIONGUARD_API_KEY\", \"Oq62vYfSJRwjnFQcUnJy5PM3SRVejYtJCXWSxnfv\")\n",
    "\n",
    "# Check if the API key is being set correctly\n",
    "print(f\"API Key: {api_key}\")\n",
    "\n",
    "# Initialize the PredictionGuard client\n",
    "client = PredictionGuard(api_key=api_key)\n",
    "\n",
    "# System Behavior\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": (\n",
    "        \"You are a beer enthusiast. Your task is to look through words and pick which of them you'd consider to be an attribute of beer. \\n\"\n",
    "        \"Do not provide any explanations or contextual information. Only return the word if it's an attribute of beer.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Only consider words that are present more than 100 times\n",
    "countsdf_100 = countsdf[countsdf['frequency'] > 100]\n",
    "\n",
    "# Define the list to store words that are considered attributes\n",
    "attributes = []\n",
    "\n",
    "def process_message(row):\n",
    "    try:\n",
    "        user_message = row['words']\n",
    "\n",
    "        # Prepare the messages list for the chatbot\n",
    "        messages = [\n",
    "            system_message,\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{user_message}\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Send the message to the PredictionGuard API\n",
    "        result = client.chat.completions.create(\n",
    "            model=\"Hermes-3-Llama-3.1-8B\",\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "        # Extract the chatbot's response\n",
    "        response = result['choices'][0]['message']['content'].strip().lower()\n",
    "\n",
    "        # Check if the response is 'yes' and add to attributes list\n",
    "        if response == 'yes':\n",
    "            attributes.append(user_message)\n",
    "        \n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing word {user_message}: {str(e)}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "    \n",
    "# Apply the process_message function to each row\n",
    "countsdf_100['response'] = countsdf_100.apply(lambda row: process_message(row), axis=1)\n",
    "\n",
    "# Debug print the list of attributes\n",
    "print(\"Attributes considered as beer-related:\", attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 Attributes\n",
    "attributes_20 = attributes[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              aroma   balance  balanced    barrel      beer    bodied  \\\n",
      "aroma      0.000000  0.354258  0.236642  0.290704  0.264807  0.151313   \n",
      "balance    0.354258  0.000000  0.554760  0.719619  0.573867  0.367685   \n",
      "balanced   0.236642  0.554760  0.000000  0.523259  0.497568  0.296343   \n",
      "barrel     0.290704  0.719619  0.523259  0.000000  0.609843  0.243495   \n",
      "beer       0.264807  0.573867  0.497568  0.609843  0.000000  0.208151   \n",
      "bodied     0.151313  0.367685  0.296343  0.243495  0.208151  0.000000   \n",
      "bottle     0.121661  0.255030  0.215697  0.303513  0.238001  0.123220   \n",
      "brown      0.317886  0.586688  0.475355  0.710690  0.483532  0.341422   \n",
      "caramel    0.140644  0.336322  0.309968  0.398645  0.295979  0.152764   \n",
      "colour     0.234351  0.271367  0.307819  0.237529  0.265351  0.280255   \n",
      "dark       0.331042  0.587629  0.505983  0.762764  0.493112  0.346348   \n",
      "flavours   0.189706  0.507986  0.460978  0.666964  0.357642  0.116583   \n",
      "malt       0.206353  0.544063  0.395360  0.511566  0.365751  0.321912   \n",
      "malty      0.249438  0.534348  0.771433  0.372048  0.408501  0.111485   \n",
      "mouthfeel  0.180078  0.436421  0.238682  0.341073  0.310915  0.071542   \n",
      "oily       0.257247  0.497049  0.527046  0.579304  0.363466  0.297582   \n",
      "rich       0.265546  0.616565  0.407976  0.674603  0.422027  0.303219   \n",
      "roasted    0.331605  0.533609  0.779308  0.780883  0.545259  0.413315   \n",
      "taste      0.218265  0.191160  0.228560  0.243071  0.292003  0.041500   \n",
      "thick      0.267348  0.500735  0.369198  0.725927  0.479841  0.265750   \n",
      "\n",
      "             bottle     brown   caramel    colour      dark  flavours  \\\n",
      "aroma      0.121661  0.317886  0.140644  0.234351  0.331042  0.189706   \n",
      "balance    0.255030  0.586688  0.336322  0.271367  0.587629  0.507986   \n",
      "balanced   0.215697  0.475355  0.309968  0.307819  0.505983  0.460978   \n",
      "barrel     0.303513  0.710690  0.398645  0.237529  0.762764  0.666964   \n",
      "beer       0.238001  0.483532  0.295979  0.265351  0.493112  0.357642   \n",
      "bodied     0.123220  0.341422  0.152764  0.280255  0.346348  0.116583   \n",
      "bottle     0.000000  0.307969  0.143137  0.223186  0.313220  0.202158   \n",
      "brown      0.307969  0.000000  0.456169  0.674323  1.238518  0.656396   \n",
      "caramel    0.143137  0.456169  0.000000  0.293140  0.401730  0.337689   \n",
      "colour     0.223186  0.674323  0.293140  0.000000  0.670492  1.144375   \n",
      "dark       0.313220  1.238518  0.401730  0.670492  0.000000  0.482741   \n",
      "flavours   0.202158  0.656396  0.337689  1.144375  0.482741  0.000000   \n",
      "malt       0.160131  0.642176  0.353195  0.328299  0.552033  0.512133   \n",
      "malty      0.180429  0.663903  0.464199  0.781664  0.607813  0.146324   \n",
      "mouthfeel  0.140596  0.325340  0.155419  0.175564  0.350548  0.187798   \n",
      "oily       0.295274  0.789404  0.296299  0.521616  0.835166  0.455672   \n",
      "rich       0.255105  0.787942  0.365954  0.354331  0.871912  0.397975   \n",
      "roasted    0.291988  1.312711  0.426775  0.858641  1.252938  0.401834   \n",
      "taste      0.122678  0.245211  0.148112  0.207838  0.326498  0.140062   \n",
      "thick      0.277320  0.656358  0.296461  0.423015  0.686018  0.377078   \n",
      "\n",
      "               malt     malty  mouthfeel      oily      rich   roasted  \\\n",
      "aroma      0.206353  0.249438   0.180078  0.257247  0.265546  0.331605   \n",
      "balance    0.544063  0.534348   0.436421  0.497049  0.616565  0.533609   \n",
      "balanced   0.395360  0.771433   0.238682  0.527046  0.407976  0.779308   \n",
      "barrel     0.511566  0.372048   0.341073  0.579304  0.674603  0.780883   \n",
      "beer       0.365751  0.408501   0.310915  0.363466  0.422027  0.545259   \n",
      "bodied     0.321912  0.111485   0.071542  0.297582  0.303219  0.413315   \n",
      "bottle     0.160131  0.180429   0.140596  0.295274  0.255105  0.291988   \n",
      "brown      0.642176  0.663903   0.325340  0.789404  0.787942  1.312711   \n",
      "caramel    0.353195  0.464199   0.155419  0.296299  0.365954  0.426775   \n",
      "colour     0.328299  0.781664   0.175564  0.521616  0.354331  0.858641   \n",
      "dark       0.552033  0.607813   0.350548  0.835166  0.871912  1.252938   \n",
      "flavours   0.512133  0.146324   0.187798  0.455672  0.397975  0.401834   \n",
      "malt       0.000000  0.318329   0.243563  0.359489  0.540199  1.055756   \n",
      "malty      0.318329  0.000000   0.224482  0.746991  0.570856  0.999078   \n",
      "mouthfeel  0.243563  0.224482   0.000000  0.369507  0.305275  0.376047   \n",
      "oily       0.359489  0.746991   0.369507  0.000000  0.639604  0.991502   \n",
      "rich       0.540199  0.570856   0.305275  0.639604  0.000000  0.807066   \n",
      "roasted    1.055756  0.999078   0.376047  0.991502  0.807066  0.000000   \n",
      "taste      0.145843  0.193465   0.164738  0.238342  0.263095  0.351471   \n",
      "thick      0.315514  0.486793   0.364450  0.625626  0.800933  0.772390   \n",
      "\n",
      "              taste     thick  \n",
      "aroma      0.218265  0.267348  \n",
      "balance    0.191160  0.500735  \n",
      "balanced   0.228560  0.369198  \n",
      "barrel     0.243071  0.725927  \n",
      "beer       0.292003  0.479841  \n",
      "bodied     0.041500  0.265750  \n",
      "bottle     0.122678  0.277320  \n",
      "brown      0.245211  0.656358  \n",
      "caramel    0.148112  0.296461  \n",
      "colour     0.207838  0.423015  \n",
      "dark       0.326498  0.686018  \n",
      "flavours   0.140062  0.377078  \n",
      "malt       0.145843  0.315514  \n",
      "malty      0.193465  0.486793  \n",
      "mouthfeel  0.164738  0.364450  \n",
      "oily       0.238342  0.625626  \n",
      "rich       0.263095  0.800933  \n",
      "roasted    0.351471  0.772390  \n",
      "taste      0.000000  0.278044  \n",
      "thick      0.278044  0.000000  \n"
     ]
    }
   ],
   "source": [
    "# Lift Analysis of Attributes\n",
    "\n",
    "# Initializations\n",
    "attribute_counter = Counter()\n",
    "co_occurrence_counter = Counter()\n",
    "lift_results = []\n",
    "total_messages = len(data)\n",
    "\n",
    "# Function to find attributes in a message\n",
    "def find_attributes(message, attribute_set):\n",
    "    words = re.findall(r'\\w+', message.lower())\n",
    "\n",
    "    # Filter the replaced words for the list\n",
    "    filtered_attributes = set([word for word in words if word not in stop_words and word in attribute_set])\n",
    "\n",
    "    return filtered_attributes\n",
    "\n",
    "# Functino to find co-occurences \n",
    "def find_co_occurrences(message, attributes_20, distance):\n",
    "    words = message.split()\n",
    "    found_attributes = []\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if word in attributes_20:\n",
    "            found_attributes.append((word, i)) \n",
    "    \n",
    "    co_occurrences = set()\n",
    "    for (attribute1, idx1), (attribute2, idx2) in itertools.combinations(found_attributes, 2):\n",
    "        if abs(idx1 - idx2) <= distance: \n",
    "            co_occurrences.add(tuple(sorted((attribute1, attribute2)))) \n",
    "    return co_occurrences\n",
    "\n",
    "def calculate_lift(attribute1, attribute2, attribute_counter, co_occurrence_counter, total_messages):\n",
    "    P_A = attribute_counter[attribute1] / total_messages \n",
    "    P_B = attribute_counter[attribute2] / total_messages  \n",
    "    \n",
    "    # Combine counts for both (brand1, brand2) and (brand2, brand1)\n",
    "    P_AB = (co_occurrence_counter[(attribute1, attribute2)] + co_occurrence_counter[(attribute2, attribute1)]) / total_messages #if (brand1, brand2) in co_occurrence_counter or (brand2, brand1) in co_occurrence_counter else 0\n",
    "    \n",
    "    if P_A * P_B == 0: \n",
    "        return 0\n",
    "    return P_AB / (P_A * P_B)\n",
    "\n",
    "# Loop through all messages to update counters\n",
    "for message in data[messagecolumn]:\n",
    "    filtered_attributes = find_attributes(message, attributes_20)\n",
    "    \n",
    "    # Update brand counter with the filtered brands\n",
    "    attribute_counter.update(filtered_attributes)\n",
    "    \n",
    "    # Now find co-occurrences using the replaced message\n",
    "    co_occurrences = find_co_occurrences(message, attributes_20, distance=10e6)\n",
    "    co_occurrence_counter.update(co_occurrences)\n",
    "\n",
    "# Calculate lifts\n",
    "for (attribute1, attribute2) in itertools.combinations(attributes_20, 2):\n",
    "    lift = calculate_lift(attribute1, attribute2, attribute_counter, co_occurrence_counter, total_messages)\n",
    "    lift_results.append((attribute1, attribute2, lift))\n",
    "\n",
    "# Create Lift Dataframe\n",
    "lift_df = pd.DataFrame(lift_results, columns=['Attribute1', 'Attribute2', 'Lift'])\n",
    "\n",
    "# Create Lift Matrix\n",
    "lift_matrix = lift_df.pivot(index='Attribute1', columns='Attribute2', values='Lift')\n",
    "lift_matrix = lift_matrix.combine_first(lift_matrix.T)\n",
    "lift_matrix.fillna(0, inplace=True)\n",
    "\n",
    "# Print Lift Matrix\n",
    "print(lift_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task C\n",
    "\n",
    "Perform a similarity analysis using cosine similarity (without word embeddings – i.e., using the bag-of-words model) with the 3 attributes specified by the customer and the reviews. \n",
    "The similarity script should accept as input a file with the product attributes, and calculate similarity scores (between 0 and 1) between these attributes and each review. That is, the output file should have 3 columns – product_name (for each product, the product_name will repeat as many times as there are reviews of the product), product_review and similarity_score. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input File Name\n",
    "inputfile = 'translated_messages.csv'\n",
    "\n",
    "# Translated Column Messages\n",
    "messagecolumn = 'translated_messages'\n",
    "\n",
    "# Import Data\n",
    "data = pd.read_csv(inputfile)\n",
    "data = data.drop(['Rating', 'Review'], axis = 1)\n",
    "\n",
    "# Output File \n",
    "outputfile = 'CDE.bagofwords.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Attributes\n",
    "userattributes = ['thick', 'rich', 'bodied']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Product Review</th>\n",
       "      <th>Cosine Similarity</th>\n",
       "      <th>Cosine Similarity TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...</td>\n",
       "      <td>You need personal informations from companies,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...</td>\n",
       "      <td>Bottle after MBCC 2024. Black colour, malty ar...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...</td>\n",
       "      <td>Thank you for sharing this Chris - Black with ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...</td>\n",
       "      <td>Boxed beer at home, proper glassware. Pitch bl...</td>\n",
       "      <td>0.187317</td>\n",
       "      <td>0.134447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...</td>\n",
       "      <td>From backlog. (As 2018 Vintage) 0,3 litre Bott...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8226</th>\n",
       "      <td>Superstition Blue Berry White🇺🇸Mead - Melomel ...</td>\n",
       "      <td>Bottle 82/100. Pours a deep, inky purple. More...</td>\n",
       "      <td>0.102062</td>\n",
       "      <td>0.059846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8227</th>\n",
       "      <td>Superstition Blue Berry White🇺🇸Mead - Melomel ...</td>\n",
       "      <td>Pours a deep berry hue. Aroma of rich blueberr...</td>\n",
       "      <td>0.154303</td>\n",
       "      <td>0.091090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8228</th>\n",
       "      <td>Superstition Blue Berry White🇺🇸Mead - Melomel ...</td>\n",
       "      <td>If you look at my reviews, you will see how hi...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8229</th>\n",
       "      <td>Superstition Blue Berry White🇺🇸Mead - Melomel ...</td>\n",
       "      <td>Huge thank you to Dakine for sharing this with...</td>\n",
       "      <td>0.121268</td>\n",
       "      <td>0.071261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8230</th>\n",
       "      <td>Superstition Blue Berry White🇺🇸Mead - Melomel ...</td>\n",
       "      <td>Deep red/purple. Oh my. The berries, white cho...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8231 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Product Name                                     Product Review  Cosine Similarity  Cosine Similarity TFIDF\n",
       "0     Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...  You need personal informations from companies,...           0.000000                 0.000000\n",
       "1     Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...  Bottle after MBCC 2024. Black colour, malty ar...           0.000000                 0.000000\n",
       "2     Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...  Thank you for sharing this Chris - Black with ...           0.000000                 0.000000\n",
       "3     Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...  Boxed beer at home, proper glassware. Pitch bl...           0.187317                 0.134447\n",
       "4     Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...  From backlog. (As 2018 Vintage) 0,3 litre Bott...           0.000000                 0.000000\n",
       "...                                                 ...                                                ...                ...                      ...\n",
       "8226  Superstition Blue Berry White🇺🇸Mead - Melomel ...  Bottle 82/100. Pours a deep, inky purple. More...           0.102062                 0.059846\n",
       "8227  Superstition Blue Berry White🇺🇸Mead - Melomel ...  Pours a deep berry hue. Aroma of rich blueberr...           0.154303                 0.091090\n",
       "8228  Superstition Blue Berry White🇺🇸Mead - Melomel ...  If you look at my reviews, you will see how hi...           0.000000                 0.000000\n",
       "8229  Superstition Blue Berry White🇺🇸Mead - Melomel ...  Huge thank you to Dakine for sharing this with...           0.121268                 0.071261\n",
       "8230  Superstition Blue Berry White🇺🇸Mead - Melomel ...  Deep red/purple. Oh my. The berries, white cho...           0.000000                 0.000000\n",
       "\n",
       "[8231 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing Dataframe\n",
    "bagofwords = pd.DataFrame()\n",
    "bagofwords['Product Name'] = data['Beer Name']\n",
    "bagofwords['Product Review'] = data['translated_messages']\n",
    "\n",
    "\n",
    "# Joining the Attributes\n",
    "text1 = ' '.join(userattributes)\n",
    "\n",
    "# Initializing the Lists\n",
    "similarity_scores = []\n",
    "similarity_scorestfidf = []\n",
    "\n",
    "# Calculating Similarity\n",
    "for text2 in data[messagecolumn]:\n",
    "    documents =[text1, text2]\n",
    "    \n",
    "    # Non-Normalized\n",
    "    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "    sparse_matrix = count_vectorizer.fit_transform(documents)\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    df = pd.DataFrame(doc_term_matrix,\n",
    "        columns=count_vectorizer.get_feature_names_out(),\n",
    "        index=['x', 'y'])\n",
    "    similarity_scores.append(cosine_similarity(df, df)[0,1])\n",
    "\n",
    "    # Normalized\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "    sparse_matrixtfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "    doc_term_matrixtfidf = sparse_matrixtfidf.todense()\n",
    "    dftfidf = pd.DataFrame(doc_term_matrixtfidf, \n",
    "        columns = tfidf_vectorizer.get_feature_names_out(),\n",
    "        index = ['x', 'y'])\n",
    "    similarity_scorestfidf.append(cosine_similarity(dftfidf, dftfidf)[0,1])\n",
    "\n",
    "# Saving to Dataframe\n",
    "bagofwords['Cosine Similarity'] = similarity_scores\n",
    "bagofwords['Cosine Similarity TFIDF'] = similarity_scorestfidf\n",
    "bagofwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputting CSV\n",
    "bagofwords.to_csv(outputfile, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task D\n",
    "\n",
    "For every review, perform a sentiment analysis (using VADER or any LLM). In case you have to change the default values of words in the VADER lexicon, use this article: https://medium.com/swlh/adding-context-to-unsupervised-sentiment-analysis-7b6693d2c9f8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predictionguard import PredictionGuard\n",
    "import os\n",
    "import pandas as pd\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input File Name\n",
    "inputfile = 'CDE.bagofwords.csv'\n",
    "\n",
    "# Output File Name\n",
    "outputfile = 'CDE.bagofwords.csv'\n",
    "\n",
    "# Translated Column Messages\n",
    "messagecolumn = 'Product Review'\n",
    "\n",
    "# Import Data\n",
    "data = pd.read_csv(inputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API key\n",
    "api_key = os.getenv(\"PREDICTIONGUARD_API_KEY\", \"Oq62vYfSJRwjnFQcUnJy5PM3SRVejYtJCXWSxnfv\")\n",
    "\n",
    "# Initialize the PredictionGuard client\n",
    "client = PredictionGuard(api_key=api_key)\n",
    "\n",
    "# System Behavior\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": (\n",
    "        \"You are our personal sentiment score evaluator for beers. Your task is to assess the product reviews and assign a sentiment score value between -1 and 1. \\n\"\n",
    "        \"Please account for potential sarcasm and slang terms. \\n\"\n",
    "        \"The context is beer, so please keep in mind how word meanings and their sentiments may change in this context. \\n\"\n",
    "        \"Do not provide any explanations or contextual information. Only return the sentiment score of the message.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "def process_message(row):\n",
    "    try:\n",
    "        user_message = row\n",
    "        # Prepare the messages list for the chatbot\n",
    "        messages = [\n",
    "            system_message,\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{user_message}\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Send the message to the PredictionGuard API\n",
    "        result = client.chat.completions.create(\n",
    "            model=\"Hermes-3-Llama-3.1-8B\",\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        # Extract the chatbot's response\n",
    "        response = result['choices'][0]['message']['content']\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "    \n",
    "# Process in Parallel \n",
    "def process_in_parallel(data_column, max_workers=5):\n",
    "    # Counter for processed messages\n",
    "    count = 0\n",
    "    \n",
    "    # Function to update the counter and print progress\n",
    "    def process_message_with_checker(row):\n",
    "        nonlocal count\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Processed {count} messages\")\n",
    "        return process_message(row)\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        responses = list(executor.map(process_message_with_checker, data_column))\n",
    "    return responses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 messages\n",
      "Processed 200 messages\n",
      "Processed 300 messages\n",
      "Processed 400 messages\n",
      "Processed 500 messages\n",
      "Processed 600 messages\n",
      "Processed 700 messages\n",
      "Processed 800 messages\n",
      "Processed 900 messages\n",
      "Processed 1000 messages\n",
      "Processed 1100 messages\n",
      "Processed 1200 messages\n",
      "Processed 1300 messages\n",
      "Processed 1400 messages\n",
      "Processed 1500 messages\n",
      "Processed 1600 messages\n",
      "Processed 1700 messages\n",
      "Processed 1800 messages\n",
      "Processed 1900 messages\n",
      "Processed 2000 messages\n",
      "Processed 2100 messages\n",
      "Processed 2200 messages\n",
      "Processed 2300 messages\n",
      "Processed 2400 messages\n",
      "Processed 2500 messages\n",
      "Processed 2600 messages\n",
      "Processed 2700 messages\n",
      "Processed 2800 messages\n",
      "Processed 2900 messages\n",
      "Processed 3000 messages\n",
      "Processed 3100 messages\n",
      "Processed 3200 messages\n",
      "Processed 3300 messages\n",
      "Processed 3400 messages\n",
      "Processed 3500 messages\n",
      "Processed 3600 messages\n",
      "Processed 3700 messages\n",
      "Processed 3800 messages\n",
      "Processed 3900 messages\n",
      "Processed 4000 messages\n",
      "Processed 4100 messages\n",
      "Processed 4200 messages\n",
      "Processed 4300 messages\n",
      "Processed 4400 messages\n",
      "Processed 4500 messages\n",
      "Processed 4600 messages\n",
      "Processed 4700 messages\n",
      "Processed 4800 messages\n",
      "Processed 4900 messages\n",
      "Processed 5000 messages\n",
      "Processed 5100 messages\n",
      "Processed 5200 messages\n",
      "Processed 5300 messages\n",
      "Processed 5400 messages\n",
      "Processed 5500 messages\n",
      "Processed 5600 messages\n",
      "Processed 5700 messages\n",
      "Processed 5800 messages\n",
      "Processed 5900 messages\n",
      "Processed 6000 messages\n",
      "Processed 6100 messages\n",
      "Processed 6200 messages\n",
      "Processed 6300 messages\n",
      "Processed 6400 messages\n",
      "Processed 6500 messages\n",
      "Processed 6600 messages\n",
      "Processed 6700 messages\n",
      "Processed 6800 messages\n",
      "Processed 6900 messages\n",
      "Processed 7000 messages\n",
      "Processed 7100 messages\n",
      "Processed 7200 messages\n",
      "Processed 7300 messages\n",
      "Processed 7400 messages\n",
      "Processed 7500 messages\n",
      "Processed 7600 messages\n",
      "Processed 7700 messages\n",
      "Processed 7800 messages\n",
      "Processed 7900 messages\n",
      "Processed 8000 messages\n",
      "Processed 8100 messages\n",
      "Processed 8200 messages\n",
      "['-0.7', '0.8', '0.7', '0.9', '0.8', '0.7', '0.95', '1', '0.95', '0.9', '0.9', '0.9', '0.9', '0.8', '0.9', '0.7', '0.9', '0.7', '0.7', '0.9', '0.9', '0.7', '0.3', '0.8', '0.8', '0.9', '0.8', '0.7', '0.8', '0.8', '0.6', '0.6', '0.6', '0.5', '0.3', '0.95', '0.9', '0.95', '0.8', '0.8', '0.9', '0.8', '0.6', '0.8', '0.8', '0.8', '0.8', '-0.5', '0.8', '0.7', '1', '0.9', '1', '0.9', '0.5', '0.3', '0.9', '0.8', '0.85', '0.8', '0.8', '0.8', '0.8', '0.8', '0.8', '0.9', '0.9', '0.8', '0.9', '0.95', '0.9', '0.7', '0.7', '0.95', '0.8', '0.6', '0.9', '0.8', '0.65', '0.8', '0.5', '0.8', '0.8', '0.8', '0.8', '0.3', '0.35', '0.8', '0.9', '0.9', '0.6', '0.6', '0.6', '0.8', '0.9', '0.9', '0.9', '0.8', '0.85', '0.95', '0.95', '0.8', '0.95', '0.3', '0.9', '0.9', '0.85', '0.8', '-0.3', '0.8', '0.8', '0.95', '0.6', '0.6', '0.9', '0.95', '0.7', '0.7', '0.7', '0.9', '0.8', '0.95', '0.8', '0.9', '0.9', '0.8', '0.7', '0.4', '0.6', '0.6', '0.9', '0.8', '0.8', '0.8', '0.8', '0.8', '0.8', '0.6', '0.3', '0.9', '0.8', '0.8', '0.8', '0.85', '0.8', '0.8', '-0.3', '0.7', '0.95', '0.8', '0.9', '0.8', '0.8', '-0.6', '-0.6', '0.8', '0.9', '0.9', '0.2', '0.9', '0.7', '0.9', '0.8', '0.8', '0.6', '0.75', '0.9', '0.9', '0.9', '0.6', '0.9', '0.9', '1', '0.8', '0.3', '0.9', '0.8', '1', '0.9', '0.9', '0.9', '0.65', '0.6', '-0.7', '0.8', '0.3', '0.5', '0.1', '0.92', '0.7', '0.8', '0.5', '0.6', '0.1', '0.7', '0.8', '0.8', '0.8', '0.8', '0.7', '0.3', '0.82', '0.8', '0.7', '0.6', '0.8', '0.8', '0.7', '0.6', '0.9', '0.6', '0.8', '0.9', '0.9', '-0.7', '0.5', '0.8', '0.9', '0.75', '0.7', '0.9', '0.8', '0.35', '0.3', '0.8', '0.8', '0.7', '0.7', '0.45', '0.7', '0.8', '0.85', '0.7', '0.9', '0.85', '0.5', '0.7', '-0.3', '0.8', '0.6', '0.6', '0.9', '0.3', '0.8', '0.5', '-0.8', '0.6', '0.5', '0.6', '0.7', '0.5', '0.8', '0.8', '0.8', '0.8', '0.9', '0.7', '0.3', '0.7', '0.9', '0.9', '0.95', '0.7', '0.8', '0.5', '0.9', '0.6', '0.7', '0.4', '0.8', '0.9', '0.95', '0.7', '0.8', '0.7', '0.8', '0.8', '0.7', '0.4', '0.4', '0.8', '0.8', '0.8', '0.8', '0.8', '0.8', '0.9', '0.9', '0.8', '0.9', '0.9', '0.6', '0.8', '0.8', '0.6', '0.8', '0.85', '1', '0.7', '-0.3', '0.8', '0.7', '0.8', '0.95', '0.9', '0.8', '0.9', '0.95', '0.6', '0.6', '0.7', '0.6', '0.9', '0.8', '0.5', '-0.5', '0.95', '0.7', '0.8', '0.8', '0.45', '0.8', '0.9', '0.7', '0.8', '0.8', '0.7', '0.8', '0.2', '0.8', '0.9', '0.8', '0.6', '0.8', '0.8', '0.9', '0.9', '0.8', '0.8', '0.85', '0.8', '0.8', '0.9', '0.1', '0.9', '0.4', '0.95', '0.6', '0.8', '0.4', '0.1', '0.95', '0.8', '0.95', '0.8', '1', '0.3', '0.7', '0.6', '0.8', '0.75', '0.3', '0.7', '0.95', '0.5', '0.8', '-0.4', '0.8', '0.8', '0.8', '0.7', '0.2', '0.8', '0.9', '0.7', '0.8', '0.8', '0.9', '0.7', '0.8', '0.7', '0.8', '0.9', '0.8', '0.7', '0.8', '0.8', '0.8', '-0.8', '0.9', '0.9', '0.9', '0.95', '0.8', '0.8\\n0.9', '0.1', '-0.3', '0.7', '0.7', '0.9', '0.8', '0.8', '0.7', '0.7', '0.9', '0.8', '0.8', '0.8', '0.6', '0.4', '0.95', '0.8', '0.8', '0.8', '0.95', '0.8', '0.9', '-0.45', '0.6', '0.6', '0.8', '0.9', '0.7', '0.3', '0.6', '0.8', '0.8', '0.8', '0.8', '0.8', '0.6', '0.8', '0.8', '0.8', '0.8', '0.8', '0.5', '0.5', '0.3', '0.2', '0.8', '0.8', '0.15', '0.8', '0.85', '0.8', '0.2', '0.3', '0.7', '0.8', '0.8', '0.8', '0.8', '0.7', '0.7', '0.8', '0.6', '0.8', '0.9', '0.8', '0.9', '0.8', '0.95', '0.8', '0.9', '0.65', '0.8', '0.7', '0.9', '1', '0.9', '0.7', '0.9', '0.7', '0.7', '0.7', '0.7', '0.7', '0.6', '0.8', '0.8', '0.1', '0.9', '0.95', '0.35', '0.95', '0.8', '0.9', '0.95', '0.9', '0.7', '0.9', '0.8', '0.8', '0.9', '0.8', '0.8', '0.8', '0.9', '0.2', '0.7', '0.9', '0.8', '0.8', '0.9', '0.95', '0.8', '0.8', '0.8', '0.8', '0.8', '0.7', '0.4', '1', '0.7', '0.1', '0.7', '0.4', '0.8', '0.8', '0.7', '0.9', '0.6', '0.8', '0.8', '0.7', '0.8', '0.9', '0.8', '0.9', '0.7', '0.8', '0.8', '0.8', '0.3', '-0.72', '0.7', '0.95', '0.45', '0.9', '0.6', '0.7', '0.6', '0.8', '0.8', '-0.4', '0.7', '0.8', '0.7', '0.2', '0.3', '0.2', '0.7', '0.9', '0.3', '0.8', '0.9', '0.9', '1', '0.8', '0.8', '0.2', '0.6', '0.9', '0.2', '0.8', '0.2', '0.8', '0.7', '0.2', '0.9', '1', '0.7', '0.2', '0.95', '0.82', '0.2', '0.3', '0.8', '0.8', '0.3', '0.8', '0.8', '0.7', '0.5', '0.6', '0.8', '0.6', '0.7', '0.2', '0.8', '0.2', '0.3', '0.7', '0.8', '0.7', '0.8', '0.6', '0.2', '0.9', '0.6', '0.8', '0.3', '0.6', '0.7', '0.9', '0.3', '0.7', '0.8', '0.8', '0.8', '0.85', '0.8', '0.8', '0.7', '0.8', '0.7', '0.8', '0.95', '0.4', '0.8', '0.2', '0.4', '0.8', '0.35', '0.8', '-0.5', '0.4', '0.5', '0.5', '0.7', '0.8', '0.5', '0.8', '0.9', '0.8', '0.9', '0.25', '0.6', '0.2', '0.8', '0.3', '0.1', '0.8', '0.9', '0.8', '0.3', '0.9', '0.3', '0.7', '0.8', '0.8', '0.81', '0.5', '0.9', '0.95', '0.3', '0.8', '-0.6', '0.8', '0.7', '0.95', '0.8', '0.6', '0.7', '0.7', '0.6', '0.4', '0.5', '0.8', '0.8', '0.9', '0.8', '0.8', '0.3', '0.3', '0.5', '0.9', '0.3', '0.8', '0.7', '0.7', '-0.6', '0.8', '0.8', '0.5', '0.8', '0.0', '0.7', '0.8', '0.6', '0.6', '0.95', '0.52', '0.7', '0.95', '0.6', '-0.9', '0.8', '0.9', '0.8', '0.1', '0.8', '0.4', '0.8', '0.6', '0.8', '0.7', '0.95', '0.7', '0.9', '0.95', '0.95', '0.8', '0.6', '0.8', '0.8', '0.9', '0.3', '0.3', '0.8', '0.9', '0.9', '0.6', '0.8', '0.9', '0.6', '0.7', '0.8', '0.7', '0.7', '0.7', '0.9', '0.8', '0.6', '0.8', '0.9', '0.9', '0.8', '0.8', '0.7', '0.95', '0.2', '0.8', '0.8', '0.7', '0.1', '0.7', '0.8', '0.8', '0.8', '0.7', '0.7', '0.9', '0.8', '0.6', '0.1', '0.8', '0.9', '0.8', '0.8', '0.95', '0.7', '0.6', '0.8', '0.7', '0.7', '0.95', '0.9', '0.8', '0.7', '0.8', '0.95', '0.8', '0.8', '0.8', '0.4', '0.8', '0.8', '0.6', '0.7', '0.85', '0.7', '0.8', '0.8', '0.8', '0.9', '0.95', '0.6', '0.8', '0.8', '0.8', '0.9', '0.8', '0.8', '0.8', '0.8', '0.7', '0.9', '0.8', '0.95', '0.7', '0.7', '0.8', '0.9', '0.8', '0.9', '0.2', '0.9', '0.95', '0.95', '0.95', '0.9', '0.6', '0.7', '0.8', '0.8', '0.7', '0.9', '0.95', '0.8', '0.8', '0.8', '0.9', '0.9', '0.8', '0.9', '0.6', '0.95', '0.8', '0.8', '0.8', '0.8', '0.8', '0.95', '0.8', '0.9', '0.8', '0.95', '0.8', '0.9', '0.1', '0.5', '0.1', '0.9', '0.9', '0.8', '0.9', '0.95', '0.95', '0.7', '0.3', '0.5', '0.8', '0.9', '0.7', '0.9', '0.9', '0.8', '0.7', '0.85', '0.8', '0.9', '0.8', '0.95', '0.7', '0.8', '0.6', '0.8', '0.95', '0.9', '0.7', '0.8', '0.8', '0.9', '0.8', '0.8', '0.6', '0.95', '0.7', '0.7', '0.7', '0.5', '0.6', '0.5', '0.7', '0.6', '0.8', '0.8', '0.7', '0.7', '0.8', '0.8', '0.8', '0.8', '0.6', '0.35', '0.7', '0.9', '0.8', '0.95', '0.8', '0.95', '0.8', '0.95', '0.7', '0.8', '0.9', '0.8', '0.9', '0.8', '0.2', '0.9', '-0.6', '0.8', '0.95', '0.8', '0.7', '0.9', '0.8', '0.6', '0.7', '0.8', '0.9', '0.9', '0.8', '0.9', '0.7', '0.9', '0.2', '0.8', '0.7', '0.3', '0.7', '0.8', '0.8', '0.8', '0.95', '0.8', '0.8', '0.8', '0.8', '0.7', '0.4', '0.8', '0.8', '0.7', '0.8', '0.9', '0.8', '0.9', '0.9', '0.8', '0.9', '0.8', '0.8', '0.8', '0.9', '0.4', '0.8', '0.8', '0.3', '0.8', '0.8', '0.8', '0.7', '0.65', '0.95', '0.5', '0.8', '0.8', '0.95', '0.8', '0.8', '0.9', '0.95', '0.9', '0.7', '0.8', '0.8', '0.8', '0.7', '0.9', '0.7', '0.8', '0.9', '0.8', '0.8', '0.6', '0.8', '0.7', '0.95', '1', '0.8', '0.8', '0.9', '0.2', '0.8', '0.6', '0.7', '0.7', '-0.5', '0.8', '0.6', '0.8', '0.9', '0.8', '0.8', '0.9', '0.6', '0.5', '0.5', '0.6', '-0.7', '0.9', '0.3', '0.1', '0.6', '0.8', '0.9', '0.95', '0.95', '0.6', '0.2', '0.3', '0.4', '0.95', '0.7', '0.8', '0.5', '0.3', '0.9', '0.9', '0.9', '0.9', '0.9', '0.7', '0.9', '0.9', '0.95', '0.9', '0.9', '0.95', '0.6', '0.8', '0.95', '0.8', '0.7', '0.7', '0.8', '0.9', '0.9', '0.8', '0.8', '0.4', '0.7', '0.9', '0.9', '0.9', '0.8', '0.6', '0.6', '0.9', '0.8', '0.8', '0.85', '0.95', '0.9', '0.8', '0.95', '-0.1', '0.95', '0.8', '0.8', '0.8', '0.7', '0.8', '0.8', '0.95', '0.8', '0.9', '0.4', '0.8', '0.9', '0.8', '0.8', '0.9', '0.7', '0.7', '0.85', '0.8', '0.5', '0.9', '0.8', '0.6', '0.15', '0.7', '0.95', '0.9', '0.6', '0.8', '0.85', '0.9', '0.9', '0.6', '0.4', '0.1', '0.8', '0.9', '0.85', '0.95', '0.8', '-0.5', '0.3', '0.8', '0.5', '0.8', '0.6', '0.7', '0.95', '0.95', '0.7', '0.6', '0.9', '0.8', '0.8', '0.9', '0.9', '0.95', '0.85', '0.3', '0.9', '0.75', '0.4', '0.4', '0.2', '0.1', '0.8', '0.8', '0.6', '0.6', '0.85', '0.1', '0.95', '0.2', '0.1', '0.7', '0.4', '0.5', '0.5', '-0.2', '0.8', '0.9', '0.95', '0.8', '0.4', '0.8', '0.8', '0.8', '0.7', '0.7', '0.7', '0.7', '0.2', '0.7', '0.8', '0.2', '0.4', '0.7', '0.1', '0.2', '0.6', '0.8', '0.5', '0.7', '0.7', '0.8', '0.7', '0.7', '0.9', '0.15', '0.7', '0.9', '0.7', '0.7', '0.9', '0.9', '0.95', '2016 vintage: 0.3\\n2012 vintage: 0.8', '0.8', '0.3', '0.5', '0.8', '0.6', '0.8', '0.75', '0.7', '0.6', '0.8', '0.7', '0.8', '0.3', '0.9', '0.8', '0.95', '0.9', '0.4', '0.8', '0.7', '-0.6', '0.9', '0.4', '0.7', '0.7', '0.6', '0.1', '-0.5', '0.8', '0.8', '0.9', '0.8', '0.8', '0.9', '0.9', '-0.5', '0.9', '0.8', '0.5', '0.7', '0.6', '0.7', '0.7', '0.8', '0.2', '0.7', '0.8', '0.8', '0.9', '0.7', '0.95', '0.7', '0.2', '0.8', '0.6', '0.8', '0.8', '0.2', '0.95', '0.7', '0.5', '0.8', '-0.5', '0.7', '0.8', '0.9', '-0.6', '0.2', '0.8', '0.8', '0.9', '0.9', '0.9', '0.9', '0.8', '-0.7', '0.8', '0.8', '0.9', '0.9', '0.6', '0.7', '0.7', '0.9', '0.8', '0.9', '0.4', '0.8', '0.2', '0.8', '0.6', '0.8', '0.8', '0.8', '0.6', '0.5', '0.85', '0.6', '0.7', '0.8', '0.7', '0.6', '0.8', '0.3', '0.3', '0.8', '0.7', '0.8', '0.3', '0.8', '0.8', '0.6', '0.6', '0.85', '0.95', '0.5', '0.8', '0.2', '0.7', '0.95', '0.8', '0.8', '0.8', '0.6', '0.5', '0.7', '0.6', '0.9', '-0.4', '0.9', '1', '0.8', '0.7', '-0.4', '0.8', '0.8', '0.5', '0.9', '0.7', '0.9', '0.9', '0.3', '0.6', '0.8', '0.8', '0.95', '0.8', '0.95', '0.2', '0.85', '0.9', '0.95', '0.8', '0.8', '0.3', '0.6', '0.95', '0.4', '0.8', '0.7', '0.95', '0.8', '0.8', '0.95', '0.8', '0.9', '0.7', '0.8', '0.95', '0.9', '0.9', '0.8', '0.8', '0.9', '0.6', '0.7', '0.3', '0.8', '0.2', '0.8', '0.7', '0.7', '0.6', '0.25', '0.95', '0.6', '0.8', '0.95', '0.8', '0.65', '0.8', '0.8', '0.8', '0.6', '0.8', '0.8', '0.8', '0.8', '0.7', '0.8', '0.5', '0.6', '0.9', '0.8', '0.8', '0.8', '0.8', '0.8', '0.8', '0.8', '0.6', '0.6', '0.8', '0.5', '0.9', '0.7', '-0.8', '0.7', '0.8', '0.7', '0.9', '0.9', '0.6', '0.7', '0.7', '0.8', '0.7', '0.8', '0.7', '0.9', '0.8', '0.95', '0.7', '0.6', '0.9', '0.8', '0.6', '0.8', '0.9', '0.6', '0.8', '0.8', '0.8', '0.7', '0.8', '0.7', '0.4', '0.8', '0.8', '0.9', '0.95', '0.8', '0.8', '0.7', '0.8', '0.8', '0.6', '0.5', '0.8', '0.7', '-0.8', '0.7', '0.9', '0.6', '0.1', '0.7', '0.9', '0.6', '0.8', '0.7', '0.8', '0.5', '0.8', '0.9', '0.8', '0.6', '0.9', '0.7', '0.5', '0.8', '0.8', '0.7', '0.9', '0.8', '0.9', '0.3', '0.6', '0.6', '0.8', '0.7', '0.3', '0.9', '0.8', '0.2', '0.3', '0.8', '0.6', '0.65', '0.3', '0.95', '-0.6', '0.6', 'Batch 3 sentiment score: 0.7\\nBatch 4 sentiment score: 0.5', '0.7', '0.5', '0.8', '-0.8', '0.5', '0.8', '0.2', '-0.5', '0.3', '0.9', '0.95', '0.3', '-0.6', '0.1', '0.95', '0.8', '0.7', '0.9', '0.7', '0.7', '0.7', '0.3', '0.8', '0.8', '0.9', '0.8', '0.85', '0.95', '0.8', '0.9', '0.8', '0.8', '0.2', '0.95', '0.8', '0.7', '0.8', '0.6', '0.6', '0.3', '-0.7', '0.8', '0.9', '0.95', '0.8', '0.5', '0.73', '0.8', '0.7', '0.9', '0.6', '0.5', '0.7', '0.8', '0.9', '0.9', '-0.8', '0.8', '0.3', '0.3', '0.7', '0.6', '0.7', '0.6', '0.4', '0.9', '0.6', '0.95', '0.95', '0.8', '0.5', '0.8', '0.8', '0.7', '0.9', '0.8', '0.9', '0.9', '0.6', '0.8', '0.6', '0.8', '0.6', '0.8', '0.5', '0.8', '0.8', '0.95', '0.7', '0.3', '-0.5', '0.9', '0.8', '0.5', '0.7', '0.8', '0.5', '0.95', '0.9', '0.95', '0.8', '0.6', '0.8', '-0.3', '0.9', '0.8', '0.75', '0.9', '0.95', '0.8', '0.2', '0.25', '0.95', '0.7', '0.9', '0.8', '1', '0.95', '0.7', '0.3', '0.8', '0.9', '0.8', '-0.5', '0.8', '0.8', '0.9', '0.8', '0.7', '0.8', '0.8', '0.8', '0.9', '0.95', '0.7', '0.9', '0.9', '0.9', '0.9', '0.6', '0.8', '0.8', '0.7', '0.9', '0.7', '1', '0.95', '0.95', '0.95', '0.8', '0.7', '0.5', '0.8', '0.4', '0.7', '0.8', '0.6', '0.9', '0.9', '0.8', '0.7', '0.7', '0.2', '0.8', '0.8', '0.9', '0.3', '0.8', '0.9', '0.95', '0.7', '0.6', '0.8', '-0.7', '0.7', '0.7', '0.7', '0.6', '0.7', '0.8', '0.9', '0.5', '0.8', '0.8', '-0.5', '0.2', '0.6', '0.2', '0.4', '0.5', '-0.3', '0.2', '0.6', '0.6', '0.8', '0.7', '0.7', '0.25', '-0.3', '0.8', '0.8', '0.8', '0.8', '0.8', '0.4', '0.6', '0.3', '0.5', '0.6', '0.7', '0.9', '0.8', '0.9', '0.8', '0.8', '0.2', '0.7', '0.8', '0.1', '0.7', '0.4', '0.7', '0.8', '0.8', '0.8', '0.5', '0.2', '0.8', '0.9', '0.8', '0.7', '0.4', '0.8', '0.8', '0.8', '0.9', '0.8', '0.9', '0.3', '0.8', '0.7', '0.8', '0.8', '0.5', '0.7', '0.8', '0.1', '0.8', '1', '0.8', '0.8', '0.8', '0.5', '0.8', '0.8', '0.8', '0.1', '0.2', '0.3', '0.5', '0.9', '0.8', '0.7', '0.7', '0.8', '0.3', '0.8', '0.8', '0.8', '0.8', '0.95', '0.5', '0.9', '0.2', '0.8', '0.8', '0.8', '0.5', '0.3', '0.3', '0.8', '0.9', '0.8', '0.8', '-0.7', '0.8', '0.4', '0.8', '0.7', '0.8', '0.5', '-0.8', '0.7', '0.8', '0.3', '0.7', '0.6', '0.15', '0.8', '0.8', '0.8', '0.8', '0.5', '0.8', '0.3', '0.9', '0.3', '-0.6', '-0.35', '0.7', '0.7', '0.4', '0.4', '0.85', '0.8', '0.8', '0.8', '0.2', '0.7', '0.9', '0.65', '0.75', '0.8', '0.7', '0.2', '0.3', '0.8', '0.8', '0.9', '0.7', '0.95', '0.2', '0.8', '0.8', '0.7', '0.7', '0.7', '0.2', '0.7', '0.7', '0.8', '0.6', '0.3', '0.8', '0.9', '0.7', '0.7', '0.6', '0.8', '0.85', '0.9', '0.9', '0.8', '0.95', '0.8', '0.8', '0.7', '0.8', '0.8', '0.6', '0.8', '0.8', '0.8', '0.5', '0.75', '0.5', '0.45', '0.7', '0.7', '0.9', '0.95', '0.8', '0.8', '0.8', '0.7', '0.8', '0.3', '0.85', '0.6', '0.8', '0.8', '0.8', '0.8', '0.7', '0.2', '0.7', '0.9', '0.2', '0.7', '0.45', '0.8', '0.7', '0.7', '0.8', '0.85', '0.35', '0.5', '0.3', '0.5', '0.7', '0.8', '0.3', '0.95', '0.5', '0.9', '-0.4', '0.95', '0.6', '0.8', '0.8', '0.9', '0.8', '0.4', '0.6', '0.8', '0.5', '0.2', '0.7', '0.9', '0.8', '0.8', '0.8', '-0.5', '0.7', '-0.65', '0.1', '0.2', '-0.7', '0.7', '0.7', '-0.5', '0.3', '0.3', '0.6', '0.7', '0.7', '0.8', '0.7', '0.85', '0.7', '0.5', '0.1', '0.6', '0.5', '-0.6', '0.8', '0.7', '0.6', '0.3', '0.6', '0.7', '0.6', '0.7', '0.6', '0.6', '0.5', '0.8', '0.85', '0.7', '0.7', '0.7', '0.8', '0.8', '0.2', '0.2', '0.8', '0.7', '0.8', '0.8', '0.8', '0.6', '0.9', '0.7', '0.2', '0.95', '0.95', '0.8', '0.95', '0.2', '0.5', '0.1', '0.7', '0.6', '0.7', '0.7', '0.9', '0.8', '0.9', '0.9', '0.2', '0.7', '-0.8', '0.85', '0.2', '0.7', '0.65', '0.95', '0.8', '0.9', '0.7', '-0.3', '0.8', '0.9', '0.9', '0.9', '0.8', '0.4', '0.9', '0.7', '-0.7', '0.75', '0.8', '0.95', '0.8', '0.8', '0.7', '0.8', '0.8', '0.8', '0.3', '1', '0.9', '0.8', '0.6', '0.7', '0.6', '0.8', '0.45', '-0.6', '0.2', '0.9', '0.7', '0.6', '-0.7', '-0.65', '0.3', '0.7', '0.8', '0.2', '0.9', '0.7', '0.8', '0.25', '0.8', '0.5', '0.1', '0.8', '-0.6', '0.8', '0.8', '0.8', '0.9', '0.8', '0.8', '0.8', '0.5', '0.8', '0.7', '0.25', '0.6', '0.3', '0.85', '0.9', '0.9', '0.5', '0.8', '0.7', '-0.3', '0.85', '-0.5', '0.95', '0.7', '0.7', '0.8', '0.8', '0.8', '0.6', '0.8', '0.75', '0.7', '0.6', '0.8', '0.7', '0.8', '0.8', '0.8', '0.95', '0.7', '0.8', '0.3', '0.9', '0.95', '0.7', '-0.2', '0.9', '0.8', '0.8', '0.8', '0.9', '0.8', '0.9', '0.9', '0.8', '0.9', '0.8', '0.8', '0.95', '1', '0.8', '0.8', '0.7', '0.8', '0.7', '0.7', '0.7', '0.55', '0.3', '0.8', '0.8', '0.8', '0.7', '0.8', '0.8', '0.9', '0.8', '0.8', '0.6', '0.9', '0.95', '0.6', '0.7', '0.8', '0.6', '-0.3', '0.9', '0.8', '0.8', '0.5', '0.8', '0.7', '0.8', '-0.2', '0.9', '0.8', '0.7', '0.85', '0.7', '0.8', '0.8', '0.9', '0.7', '0.8', '0.85', '0.95', '0.8', '0.9', '0.9', '0.7', '0.95', '0.6', '0.5', '0.7', '0.7', '0.2', '0.8', '0.8', '0.8', '0.1', '0.8', '0.8', '0.9', '0.8', '0.9', '0.8', '0.8', '0.7', '0.8', '0.9', '0.75', '0.2', '0.8', '0.7', '0.8', '0.6', '0.8', '0.8', '0.8', '0.7', '0.7', '0.8', '0.8', '0.9', '0.8', '0.95', '0.8', '0.9', '0.7', '0.8', '0.6', '0.9', '0.9', '0.9', '0.9', '0.9', '0.8', '0.6', '0.7', '0.4', '0.35', '0.4', '0.5', '0.7', '0.8', '0.9', '0.8', '0.8', '0.7', '0.7', '0.7', '0.7', '0.9', '0.8', '0.8', '0.9', '0.8', '0.8', '0.9', '0.8', '0.8', '0.8', '0.6', '0.3', '0.9', '0.3', '0.7', '0.8', '0.8', '0.7', '0.7', '0.9', '0.8', '0.3', '0.9', '0.9', '0.8', '0.7', '0.7', '0.8', '0.7', '0.8', '0.9', '0.8', '0.85', '0.7', '0.7', '0.6', '0.5', '0.9', '0.8', '0.5', '0.8', '0.8', '0.8', '0.95', '0.95', '0.5', '0.6', '0.8', '0.9', '0.8', '0.8', '0.4', '0.7', '0.9', '0.8', '0.6', '0.6', '0.6', '0.7', '0.9', '0.75', '0.9', '0.8', '0.95', '0.9', '0.8', '0.8', '0.8', '0.7', '0.95', '0.8', '0.7', '0.6', '0.7', '0.8', '0.5', '0.9', '0.8', '0.8', '0.7', '0.95', '0.6', '0.5', '0.95', '0.6', '0.95', '0.6', '0.8', '0.5', '0.8', '0.95', '0.9', '1', '0.9', '0.8', '0.8', '0.95', '0.4', '0.5', '0.85', '0.9', '0.8', '0.9', '0.8', '0.6', '0.9', '0.4', '-0.5', '0.8', '0.7', '0.8', '0.7', '0.8', '0.8', '0.9', '0.7', '0.6', '0.7', '0.5', '-0.8', '0.9', '0.95', '0.9', '0.9', '0.8', '0.9', '0.8', '0.4', '0.5', '0.4', '0.8', '0.7', '0.95', '0.8', '0.8', '0.2', '0.9', '0.6', '0.9', '0.6', '0.7', '0.7', '0.85', '0.8', '0.9', '0.8', '0.3', '0.8', '0.7', '0.9', '0.6', '0.6', '-0.6', '0.4', '0.6', '0.8', '0.7', '0.7', '0.6', '0.2', '0.6', '0.6', '0.7', '0.7', '0.6', '0.95', '0.4', '0.8', '0.6', '0.7', '0.8', '0.9', '0.6', '0.6', '0.9', '0.8', '0.7', '0.8', '0.6', '0.8', '0.8', '0.8', '0.6', '0.8', '0.8', '0.8', '0.5', '0.8', '0.8', '0.7', '0.2', '0.5', '0.9', '0.8', '0.6', '0.8', '0.8', '0.9', '0.95', '0.8', '0.9', '0.95', '0.6', '0.95', '0.2', '0.5', '0.8', '0.7', '0.8', '0.7', '0.9', '0.7', '0.8', '0.8', '0.7', '0.8', '0.8', '0.8', '0.8', '0.9', '0.7', '0.8', '0.6', '0.8', '0.75', '0.9', '0.95', '0.8', '0.8', '0.5', '0.8', '0.6', '0.8', '0.8', '0.6', '0.6', '0.8', '0.95', '0.8', '0.7', '0.2', '0.3', '0.6', '0.8', '0.5', '0.8', '0.9', '0.8', '0.6', '0.7', '0.8', '0.8', '0.5', '0.8', '0.3', '0.8', '0.8', '0.7', '0.3', '0.8', '0.7', '0.8', '0.9', '-0.6', '0.3', '0.95', '0.8', '0.7', '0.8', '0.8', '0.3', '0.6', '-0.6', '0.8', '0.9', '0.95', '0.6', '0.9', '0.8', '0.4', '0.8', '0.9', '0.7', '0.7', '0.8', '1', '0.8', '0.7', '0.9', '0.7', '0.7', '0.95', '0.9', '-0.6', '0.8', '0.7', '0.8', '0.9', '0.8', '0.8', '0.9', '0.8', '0.2', '0.95', '0.8', '0.7', '0.2', '0.8', '0.9', '0.8', '0.6', '0.8', '0.7', '1.0', '0.9', '0.8', '1', '0.7', '0.7', '0.8', '0.2', '0.8', '0.8', '0.3', '0.5', '0.7', '0.9', '0.2', '0.5', '0.8', '0.8', '0.8', '0.8', '0.8', '0.6', '0.8', '0.8', '0.8', '0.7', '0.7', '0.8', '-0.3', '0.7', '0.7', '0.7', '0.5', '0.7', '0.9', '0.3', '0.8', '0.7', '0.9', '0.7', '0.8', '0.6', '0.8', '0.8', '0.7', '0.8', '0.8', '0.9', '0.7', '0.8', '0.7', '0.3', '0.6', '0.5', '0.6', '0.9', '0.5', '0.3', '0.95', '0.7', '0.8', '0.6', '0.8', '0.95', '0.95', '0.8', '0.8', '0.9', '0.8', '0.7', '0.9', '0.3', '0.8', '0.95', '0.6', '0.95', '0.6', '0.7', '0.9', '0.8', '0.7', '0.8', '0.75', '0.7', '0.8', '0.8', '-0.6', '0.8', '0.8', '0.8', '0.4', '0.95', '0.95', '0.7', '0.8', '0.8', '0.8', '0.7', '0.8', '0.3', '0.8', '0.8', '0.8', '0.8', '0.8', '0.8', '0.7', '0.7', '0.95', '0.9', '0.2', '0.8', '0.8', '0.8', '0.9', '0.8', '0.95', '0.3', '0.7', '0.5', '0.5', '0.7', '0.2', '0.8', '0.6', '0.3', '0.45', '0.8', '0.5', '0.7', '-0.8', '0.8', '0.6', '0.8', '0.8', '0.7', '0.9', '0.8', '0.8', '0.6', '0.4', '0.6', '0.95', '0.2', '-0.4', '-0.3', '0.9', '0.3', '0.8', '0.8', '0.8', '0.1', '0.65', '0.7', '0.9', '0.9', '0.95', '0.4', '0.8', '0.8', '0.7', '0.8', '0.8', '0.85', '0.9', '0.95', '0.4', '0.8', '0.7', '0.9', '0.4', '0.8', '0.7', '0.7', '0.3', '0.8', '0.8', '0.7', '0.8', '0.8', '0.8', '0.7', '0.6', '0.5', '0.8', '0.7', '0.85', '0.8', '0.9', '0.4', '-0.6', '0.95', '0.5', '0.8', '0.7', '0.7', '0.8', '0.9', '0.8', '0.5', '0.4', '0.8', '0.3', '0.7', '0.6', '0.7', '0.2', '0.8', '0.8', '0.8', '0.8', '0.35', '0.9', '0.7', '0.8', '0.8', '0.8', '0.95', '0.2', '0.8', '0.8', '0.8', '0.6', '0.9', '0.7', '0.5', '0.5', '0.7', '0.6', '0.3', '0.4', '0.8', '0.95', '0.6', '0.7', '0.6', '0.8', '0.6', '0.8', '0.5', '0.6', '0.0', '0.7', '0.5', '0.8', '0.95', '0.9', '0.5', '0.6', '0.8', '0.6', '0.85', '0.8', '0.9', '0.7', '0.9', '-0.4', '0.95', '-0.7', '0.9', '0.8', '0.8', '0.9', '0.8', '0.9', '0.8', '0.8', '0.4', '0.6', '0.6', '1', '0.7', '-0.6', '0.85', '0.4', '0.85', '0.8', '0.8', '0.8', '0.7', '1', '0.8', '0.8', '0.6', '0.5', '0.8', '0.6', '0.5', '0.7', '0.8', '0.5', '0.6', '0.6', '0.1', '0.9', '0.6', '0.9', '0.65', '0.9', '0.9', '0.7', '0.9', '0.9', '0.85', '0.5', '0.8', '0.7', '0.9', '0.9', '0.8', '0.95', '0.7', '0.8', '0.8', '0.7', '0.5', '0.7', '0.8', '0.6', '0.8', '0.8', '0.5', '0.95', '0.6', '0.8', '0.7', '0.8', '0.8', '0.7', '0.8', '0.8', '0.4', '0.4', '0.95', '0.9', '0.9', '1', '0.8', '0.8', '0.2', '0.6', '0.7', '-0.8', '0.6', '0.8', '0.7', '0.7', '0.8', '0.7', '0.6', '0.7', '0.9', '0.85', '0.6', '0.9', '0.8', '0.7', '0.9', '0.7', '0.8', '0.5', '0.3', '0.8', '0.8', '0.9', '0.8', '0.7', '0.7', '0.8', '0.9', '0.9', '0.95', '0.3', '0.6', '0.7', '0.7', '0.8', '0.9', '0.6', '0.8', '0.8', '0.3', '-0.7', '0.3', '0.8', '0.8', '0.8', '0.8', '0.2', '0.5', '0.8', '0.5', '0.9', '0.9', '0.9', '0.7', '0.8', '0.3', '0.8', '0.3', '0.4', '0.8', '0.8', '-0.4', '0.7', '0.7', '1', '0.6', '0.7', '0.7', '0.4', '0.7', '0.8', '0.5', '0.7', '0.7', '0.6', '0.95', '0.9', '-0.65', '0.3', '0.9', '0.8', '0.7', '0.7', '0.6', '0.9', '0.7', '0.7', '0.8', '0.8', '0.8', '0.65', '0.5', '0.7', '0.8', '0.8', '0.8', '0.95', '0.8', '0.8', '-0.7', '0.6', '0.3', '0.8', '0.8', '0.8', '0.6', '0.8', '0.7', '0.7', '0.7', '-0.5', '0.9', '0.95', '0.7', '0.8', '0.7', '0.6', '0.8', '0.7', '0.8', '0.8', '0.9', '0.7', '0.4', '0.95', '0.8', '0.2', '0.9', '0.2', '0.9', '0.95', '0.8', '0.7', '0.8', '0.8', '0.9', '0.8', '0.9', '0.9', '0.9', '0.1', '0.8', '0.4', '0.5', '0.8', '0.8', '0.3', '0.9', '0.8', '0.7', '0.95', '0.8', '0.7', '0.8', '0.8', '0.6', '0.8', '0.8', '0.7', '0.9', '0.9', '0.8', '0.8', '0.8', '0.65', '0.7', '0.9', '0.9', '0.4', '0.9', '0.85', '0.8', '0.7', '0.9', '0.6', '0.7', '0.8', '0.9', '0.4', '0.8', '0.8', '0.8', '0.9', '0.6', '0.8', '0.8', '0.9', '0.9', '0.95', '0.8', '0.7', '0.2', '0.7', '0.8', '0.3', '0.8', '0.8', '0.6', '0', '0.7', '0.9', '0.8', '0.5', '-0.6', '0.7', '0.7', '0.6', '0.8', '0.7', '0.25', '0.7', '0.8', '0.7', '0.8', '0.6', '0.8', '0.6', '0.8', '0.7', '0.8', '0.6', '0.8', '0.6', '0.8', '0.7', '0.7', '0.7', '0.9', '0.8', '0.9', '0.7', '0.7', '0.7', '-0.8', '0.7', '0.9', '0.85', '0.95', '0.8', '0.5', '-0.3', '0.8', '0.9', '0.9', '0.8', '0.3', '0.9', '0.8', '0.9', '0.8', '0.8', '0.4', '0.7', '0.3', '0.8', '0.3', '0.8', '0.7', '0.4', '0.8', '0.95', '0.9', '0.8', '0.8', '0.8', '0.7', '0.7', '0.8', '0.9', '0.6', '0.3', '0.7', '0.8', '0.3', '0.8', '0.6', '0.3', '0.3', '0.6', '0.6', '0.9', '0.8', '0.8', '0.7', '0.45', '0.8', '-0.4', '0.7', '0.7', '0.8', '0.8', '0.7', '0.8', '0.4', '0.8', '0.8', '0.3', '0.6', '0.35', '0.9', '0.8', '0.7', '0.8', '0.8', '0.8', '0.7', '0.8', '0.8', '0.9', '0.7', '0.2', '0.75', '0.8', '0.7', '0.9', '0.8', '0.9', '0.8', '0.7', '0.6', '0.7', '0.7', '0.7', '0.7', '-0.6', '0.8', '0.7', '0.3', '0.7', '0.9', '0.6', '0.7', '0.8', '0.9', '0.8', '0.8', '0.8', '0.3', '0.7', '0.6', '0.6', '0.7', '0.25', '-0.6', '0.2', '0.9', '0.8', '0.8', '0.8', '0.7', '-0.5', '0.8', '0.9', '0.6', '0.9', '0.85', '0.35', '0.3', '0.95', '0.8', '0.3', '0.9', '0.7', '0.95', '0.9', '0.5', '0.8', '0', '0.6', '0.9', '0.8', '0.95', '0.95', '0.8', '0.8', '0.6', '0.7', '0.9', '0.7', '0.8', '0.5', '0.6', '0.2', '0.2', '0.6', '0.58', '0.7', '0.2', '0.8', '0.6', '0.8', '0.8', '0.8', '0.7', '0.6', '0.8', '-0.6', '0.8', '0.3', '0.6', '0.8', '0.7', '0.8', '0.7', '0.7', '0.7', '0.8', '0.9', '0.8', '0.8', '0.7', '0.7', '0.7', '0.7', '0.8', '0.7', '0.7', '0.7', '0.8', '0.9', '0.95', '0.6', '0.8', '0.7', '0.7', '0.6', '0.4', '0.8', '0.95', '0.2', '0.6', '0.8', '0.8', '0.7', '0.95', '0.7', '0.7', '0.5', '0.3', '0.7', '0.9', '0.95', '0.8', '0.9', '0.8', '0.9', '0.8', '0.8', '0.7', '0.7', '0.8', '0.1', '0.8', '-0.7', '0.4', '0.9', '0.8', '0.9', '0.8', '0.4', '0.8', '0.8', '0.25', '0.8', '0.5', '0.9', '0.9', '0.5', '0.8', '0.8', '0.5', '0.3', '0.4', '0.5', '0.9', '0.9', '0.7', '0.7', '0.9', '0.7', '0.8', '0.8', '0.8', '0.9', '0.6', '0.8', '0.8', '0.8', '0.8', '0.5', '0.95', '0.9', '-0.5', '0.5', '0.8', '0.7', '0.8', '0.8', '0.9', '0.8', '0.7', '0.25', '0.9', '0.8', '0.7', '0.8', '0.6', '0.1', '0.9', '0.3', '0.7', '0.2', '0.7', '0.8', '0.7', '-0.5', '0.9', '0.7', '0.6', '0.9', '-0.1', '0.8', '0.6', '0.8', '0.7', '0.45', '0.5', '0.2', '0.7', '0.8', '0.8', '0.95', '0.95', '0.8', '0.8', '0.3', '0.5', '0.7', '0.2', '0.9', '0.4', '0.8', '0.8', '0.3', '0.6', '0.25', '-0.45', '0.7', '0.8', '0.8', '0.7', '0.7', '0.8', '0.8', '0.8', '0.7', '0.7', '0.7', '0.5', '0.9', '0.8', '0.2', '0.7', '0.8', '0.2', '0.8', '0.8', '0.8', '0.8', '0.6', '0.8', '0.9', '0.9', '0.9', '0.9', '0.95', '0.8', '0.8', '0.7', '0.6', '0.7', '0.7', '0.6', '0.7', '0.8', '0.9', '0.7', '0.9', '0.6', '0.8', '0.7', '0.8', '0.6', '0.5', '0.8', '0.8', '0.7', '0.8', '0.4', '0.9', '0.8', '0.8', '0.4', '0.5', '0.7', '0.9', '0.8', '0.9', '0.8', '0.9', '0.8', '0.4', '0.8', '0.8', '0.6', '0.95', '0.9', '0.7', '0.8', '0.8', '0.9', '0.9', '0.95', '0.8', '0.9', '0.9', '1', '0.9', '0.9', '0.8', '0.9', '0.8', '0.8', '0.7', '0.7', '0.7', '0.9', '0.6', '0.9', '0.6', '0.7', '0.9', '0.8', '0.8', '0.8', '0.8', '0.8', '0.6', '0.6', '0.3', '0.8', '0.6', '0.8', '0.8', '0.8', '0.85', '0.8', '0.8', '0.8', '0.3', '0.6', '0.8', '0.95', '0.3', '0.7', '0.9', '0.8', '0.85', '0.8', '0.95', '0.8', '0.2', '0.7', '0.5', '0.9', '0.5', '0.3', '0.2', '0.7', '0.8', '0.2', '0.8', '0.7', '0.8', '0.8', '0.8', '0.8', '0.8', '0.7', '0.3', '0.7', '0.9', '0.8', '0.7', '0.8', '0.7', '0.8', '0.8', '0.9', '0.6', '0.8', '0.8', '0.85', '0.8', '0.9', '0.6', '0.9', '0.7', '0.8', '1', '0.5', '0.7', '0.8', '0.7', '0.9', '0.95', '0.8', '0.8', '0.5', '0.8', '0.3', '0.9', '0.5', '0.7', '0.8', '0.8', '0.8', '0.95', '0.7', '0.8', '0.8', '0.5', '0.9', '0.7', '0.7', '0.9', '0.6', '0.35', '0.6', '0.8', '0.7', '0.9', '0.8', '0.7', '0.3', '0.8', '0.8', '0.8', '0.7', '0.8', '0.8', '0.6', '0.9', '0.2', '0.8', '0.9', '0.5', '0.6', '0.8', '0.9', '0.95', '0.95', '0.9', '0.8', '0.8', '0.7', '0.9', '0.6', '0.4', '0.8', '0.98', '0.7', '0.8', '0.3', '0.7', '0.9', '0.7', '0.9', '0.9', '0.9', '0.8', '0.8', '0.8', '0.7', '0.8', '0.8', '0.8', '0.8', '0.8', '0.8', '0.2', '0.75', '0.8', '0.6', '0.9', '0.5', '0.6', '-0.7', '0.1', \"Error: HTTPSConnectionPool(host='api.predictionguard.com', port=443): Read timed out. (read timeout=None)\", \"Error: HTTPSConnectionPool(host='api.predictionguard.com', port=443): Read timed out. (read timeout=None)\", \"Error: HTTPSConnectionPool(host='api.predictionguard.com', port=443): Read timed out. (read timeout=None)\", \"Error: HTTPSConnectionPool(host='api.predictionguard.com', port=443): Read timed out. (read timeout=None)\", \"Error: HTTPSConnectionPool(host='api.predictionguard.com', port=443): Read timed out. (read timeout=None)\", '0.8', '0.8', '0.8', '0.8', '0.95', '0.85', '0.95', '0.2', '0.7', '0.7', '0.9', '0.8', '0.95', '0.8', '0.8', '0.9', '0.7', '0.8', '0.3', '0.8', '0.7', '0.4', '0.8', '0.8', '0.8', '0.8', '0.6', '0.9', '0.7', '0.5', '0.7', '0.6', '0.8', '0.8', '0.85', '0.9', '0.8', '0.8', '0.2', '0.3', '0.5', '0.8', '0.65', '0.7', '0.7', '0.2', '0.4', '0.65', '0.95', '1', '0.5', '0.8', '0.8', '1', '0.2', '0.9', '0.85', '0.8', '0.8', '0.9', '0.8', '0.5', '0.35', '0.8', '0.2', '0.3', '0.85', '0.5', '-0.5', '0.7', '0.5', '0.2', '0.2', '0.8', '0.5', '0.6', '0.6', '0.9', '0.6', '0.7', '0.8', '0.7', '0.8', '0.9', '0.9', '0.8', '0.8', '0.0', '0.4', '0.8', '0.75', '0.8', '0.9', '0.8', '0.9', '0.7', '0.6', '0.1', '0.6', '0.8', '0.9', '0.9', '0.7', '0.8', '0.2', '0.8', '0.7', '0.7', '0.8', '0.7', '0.7', '0.2', '0.6', '0.8', '0.6', '0.9', '0.8', '0.7', '0.9', '0.7', '0.9', '0.7', '0.8', '0.8', '0.4', '0.8', '0.4', '0.8', '0.6', '0.8', '0.6', '0.6', '0.8', '0.7', '0.2', '0.6', '0.6', '0.7', '0.7', '0.2', '0.8', '0.9', '0.8', '0.3', '0.6', '0.8', '0.8', '0.5', '0.7', '0.5', '0.5', '0.1', '0.7', '0.6', '0.6', '0.8', '0.7', '0.8', '0.7', '0.8', '0.7', '0.7', '0.7', '0.6', '0.75', '0.2', '0.4', '0.8', '0.8', '0.8', '0.8', '0.7', '0.4', '0.8', '0.8', '0.8', '0.8', '0.9', '0.8', '0.7', '0.9', '0.8', '0.8', '0.95', '0.9', '0.8', '0.9', '0.7', '0.8', '0.8', '0.7', '0.8', '0.7', '0.8', '0.8', '0.7', '0.6', '0.8', '0.5', '0.9', '0.1', '0.6', '0.7', '0.85', '0.7', '0.9', '0.8', '0.7', '0.7', '0.9', '0.95', '0.6', '0.8', '0.8', '0.8', '0.8', '0.8', '0.95', '0.6', '0.8', '0.7', '0.95', '0.8', '-0.2', '0.8', '0.8', '0.2', '0.8', '0.9', '0.5', '0.7', '0.8', '0.35', '0.8', '0.9', '0.7', '0.7', '0.9', '0.8', '0.8', '0.8', '-0.4', '0.3', '0.6', '0.8', '0.7', '0.85', '0.8', '0.9', '0.9', '0.8', '0.8', '0.7', '0.8', '0.95', '0.8', '-0.5', '0.8', '0.8', '0.9', '0.4', '0.5', '0.9', '0.4', '0.8', '-0.3', '0.6', '0.8', '0.3', '0.65', '0.6', '0.8', '0.8', '0.8', '0.6', '0.8', '0.8', '0.8', '0.8', '0.4', '0.7', '0.85', '0.8', '0.95', '0.1', '0.8', '0.8', '-0.5', '0.9', '0.7', '0.6', '0.4', '0.6', '0.7', '0.95', '0.8', '0.95', '0.9', '0.75', '0.6', '0.8', '0.8', '0.3', '0.5', '0.8', '0.25', '0.5', '0.9', '0.95', '0.8', '0.8', '0.6', '0.8', '0.85', '0.9', '0.8', '0.6', '0.9', '0.8', '0.9', '0.9', '0.8', '0.9', '0.9', '0.9', '0.2', '0.1', '0.5', '0.8', '0.35', '0.95', '0.7', '0.4', '-0.4', '0.4', '0.8', '0.7', '0.6', '0.7', '0.3', '0.8', '0.8', '0.7', '-0.9', '0.9', '0.8', '0.3', '0.8', '0.95', '0.8', '0.7', '0.2', '0.5', '0.4', '0.3', '0.5', '0.8', '0.8', '0.8', '0.7', '0.8', '0.9', '-0.6', '0.8', '0.8', '0.8', '0.2', '0.8', '0.8', '0.9', '0.8', '0.4', '0.05', '0.5', '0.8', '0.7', '0.7', '0.65', '0.75', '0.8', '0.6', '0.8', '0.9', '1', '0.8', '-0.8', '0.8', '0.5', '0.8', '0.3', '0.7', '0.8', '1', '0.95', '0.3', '0.9', '1', '0.8', '0.9', '0.9', '0.9', '0.8', '0.7', '0.9', '0.8', '0.7', '-0.5', '0.3', '-0.4', '0.8', '0.7', '-0.2', '0.7', '0.8', '0.9', '0.7', '0.8', '0.5', '0.8', '0.8', '0.6', '0.9', '0.6', '0.5', '0.9', '0.8', '0.8', '0.95', '0.6', '0.5', '0.8', '0.7', '0.5', '0.7', '0.85', '0.4', '0.3', '0.8', '-0.65', '-0.6', '-0.4', '0.5', '0.8', '0.7', '0.7', '0.7', '0.7', '0.95', '0.6', '0.8', '0.85', '0.8', '0.6', '0.9', '0.6', '0.8', '0.8', '0.9', '0.9', '1', '0.8', '0.9', '0.5', '0.8', '0.4', '0.8', '0.5', '0.8', '0.7', '0.9', '0.7', '0.9', '0.8', '0.8', '0.2', '0.3', '0.8', '0.8', '0.7', '0.9', '0.8', '0.1', '0.7', '0.99', '0.6', '0.8', '0.8', '0.9', '0.65', '0.45', '0.7', '0.8', '0.8', '0.9', '0.9', '0.9', '0.6', '0.6', '0.9', '0.9', '0.95', '0.9', '0.7', '0.8', '0.8', '0.7', '0.8', '0.8', '0.5', '0.1', '0.8', '0.8', '0.9', '0.7', '0.6', '0.7', '0.8', '0.2', '1', '0.8', '0.9', '0.9', '0.8', '0.9', '0.5', '0.8', '0.9', '0.95', '0.9', '0.8', '0.9', '0.9', '0.7', '0.6', '0.7', '0.8', '0.95', '0.6', '0.9', '0.8', '0.5', '0.9', '0.8', '0.7', '0.3', '0.7', '0.8', '0.3', '0.8', '0.8', '0.6', '0.95', '0.8', '0.8', '0.2', '0.7', '0.8', '0.5', '0.0', '0.8', '0.3', '0.4', '0.5', '0.6', '0.6', '0.3', '0.7', '0.7', '0.9', '0.8', '0.6', '0.8', '0.8', '0.8', '0.7', '0.4', '0.95', '0.6', '0.8', '0.7', '0.8', '0.7', '-0.7', '0.6', '0.6', '0.7', '0.3', '0.8', '0.7', '0.5', '0.7', '0.6', '0.7', '0.8', '0.9', '0.8', '0.8', '0.8', '0.5', '0.7', '0.9', '0.4', '0.9', '0.8', '0.4', '-0.6', '0.8', '0.8', '0.9', '0.9', '0.9', '0.7', '0.9', '0.8', '0.5', '0.8', '0.7', '0.9', '0.8', '0.8', '0.9', '0.5', '0.8', '0.8', '0.8', '0.3', '0.7', '0.8', '0.7', '0.9', '0.8', '0.8', '-0.4', '0.6', '0.8', '0.8', '0.8', '1', '0.7', '0.3', '0.7', '0.95', '0.72', '0.8', '0.8', 'Sentiment Score: 0.6', '0.95', '0.8', '0.8', '0.6', '-0.6', '0.7', '0.8', '0.95', '0.7', '0.8', '0.8', '0.7', '0.4', '0.2', '0.6', '0.8', '0.2', '0.8', '0.9', '0.8', '0.65', '0.8', '0.6', '0.8', '0.8', '0.7', '0.9', '0.7', '0.8', '0.6', '0.6', '0.2', '0.5', '0.95', '-0.25', '0.8', '0.8', '0.7', '0.7', '0.7', '0.6', '0.95', '0.8', '0.8', '0.8', '0.8', '0.9', '0.9', '0.8', '0.8', '0.7', '0.8', '0.95', '0.8', '0.7', '0.7', '0.6', '0.9', '0.7', '0.7', '0.9', '0.7', '0.9', '0.95', '0.6', '0.9', '0.9', '0.7', '0.8', '0.8', '0.8', '0.7', '0.7', '0.6', '0.7', '0.9', '0.7', '0.3', '0.8', '0.8', '0.8', '0.8', '0.8', '0.9', '0.9', '0.8', '0.8', '0.9', '0.8', '0.8', '0.9', '0.7', '0.8', '0.7', '0.9', '0.8', '0.6', '0.6', '0.6', '0.8', '0.75', '0.6', '0.8', '0.9', '0.7', '0.25', '0.8', '0.8', '0.9', '-0.3', '0.8', '0.8', '0.95', '0.9', '0.8', '-0.4', '0.8', '0.3', '0.7', '0.4', '0.6', '0.8', '0.8', '0.8', '0.6', '0.8', '0.8', '0.5', '0.6', '0.4', '0.7', '0.7', '0.8', '0.8', '-0.6', '0.7', '0.8', '0.8', '0.9', '0.6', '0.1', '0.6', '0.2', '0.7', '0.8', '0.9', '0.7', '0.95', '0.8', '0.8', '0.7', '0.9', '0.8', '0.8', '0.7', '0.9', '0.3', '0.8', '0.3', '0.5', '0.7', '0.6', '0.8', '0.8', '0.9', '0.7', '0.5', '0.8', '0.8', '0.5', '0.7', '0.3', '0.5', '0.8', '0.8', '0.75', '-0.25', '0.3', '0.7', '0.7', '0.6', '0.6', '0.9', '0.8', '0.7', '0.7', '0.8', '0.1', '0.7', '0.2', '0.8', '-0.7', '0.8', '0.8', '-0.3', '0.7', '0.7', '0.8', '0.8', '0.8', '0.4', '0.1', '0.6', '0.4', '0.3', '0.8', '0.8', '0.6', '-0.5', '0.6', '0.8', '0.8', '-0.7', '0.3', '0.7', '0.8', '0.8', '0.92', '0.2', '0.9', '0.5', '0.6', '0.9', '0.7', '0.9', '0.7', '0.7', '0.9', '0.8', '0.7', '0.8', '0.5', '0.8', '0.7', '0.2', '0.4', '0.95', '0.95', '0.2', '0.1', '0.8', '0.8', '0.8', '0.7', '0.8', '0.8', '0.9', '0.6', '0.95', '0.9', '0.9', '0.9', '0.95', '0.6', '0.8', '0.8', '0.8', '0.8', '0.8', '0.8', '0.95', '0.8', '0.7', '0.7', '0.8', '0.8', '0.7', '0.8', '0.95', '0.8', '0.8', '0.8', '0.8', '0.8', '0.8', '0.85', '0.9', '0.8', '0.95', '0.8', '0.8', '0.7', '0.3', '0.4', '0.7', '0.8', '0.8', '0.8', '1', '0.6', '0.8', '0.95', '0.54', '0.95', '0.8', '0.8', '0.9', '0.95', '0.75', '0.8', '0.25', '0.7', '0.7', '0.7', '0.9', '0.8', '0.7', '0.9', '0.8', '0.8', '1', '0.8', '0.8', '0.3', '0.7', '0.3', '0.9', '0.85', '0.6', '0.8', '0.8', '0.8', '0.8', '0.8', '0.8', '0.7', '0.8', '0.8', '0.7', '0.8', '0.8', '-0.7', '0.7', '0.9', '0.7', '0.7', '0.8', '0.6', '0.5', '0.7', '0.6', '0.9', '0.5', '0.8', '0.8', '0.8', '0.8', '0.7', '0.7', '-0.4', '0.9', '0.8', '0.6', '0.9', '0.8', '0.6', '0.7', '0.9', '0.8', '0.6', '-0.6', '0.3', '0.8', '0.8', '0.9', '0.8', '0.8', '0.1', '0.4', '0.9', '0.8', '0.8', '0.7', '0.8', '0.2', '0.9', '0.8', '0.7', '0.8', '0.8', '0.6', '0.7', '0.9', '0.7', '0.8', '0.8', '0.15', '-0.4', '0.7', '0.3', '0.6', '0.6', '0.8', '0.7', '0.5', '0.9', '0.6', '0.9', '0.6', '0.6', '0.9', '0.7', '0.65', '0.75', '0.7', '0.8', '0.95', '0.7', '0.8', '0.7', '0.7', '0.9', '0.8', '0.5', '0.9', '0.6', '0.8', '0.7', '0.8', '0.6', '0.9', '0.8', '0.8', '0.8', '-0.4', '0.9', '0.8', '0.8', '0.8', '0.5', '0.5', '0.7', '0.7', '0.6', '0.9', '0.7', '0.7', '0.8', '0.8', '0.9', '0.7', '0.95', '0.3', '0.8', '0.9', '-0.5', '0.7', '0.3', '0.6', '0.8', '0.8', '0.45', '0.5', '0.2', '0.9', '0.7', '0.7', '0.8', '0.6', '0.7', '0.9', '0.7', '0.8', '0.5', '0.6', '0.9', '0.6', '-0.6', '0.5', '0.6', '0.6', '0.7', '0.9', '0.6', '0.7', '0.8', '0.8', '0.8', '0.9', '0.7', '0.85', '0.8', '0.3', '0.9', '0.3', '0.7', '0.7', '0.8', '0.8', '0.8', '0.6', '0.75', '0.8', '0.7', '0.7', '0.8', '0.8', '0.8', '0.3', '0.8', '0.8', '0.3', '0.8', '0.5', '0.5', '0.9', '0.9', '0.6', '0.95', '0.8', '0.8', '0.8', '0.85', '0.9', '0.8', '0.3', '0.7', '-0.7', '0.7', '0.6', '0.7', '0.8', '0.6', '0.8', '0.8', '0.8', '0.7', '0.8', '0.85', '0.7', '0.7', '0.9', '0.8', '0.8', '0.2', '0.75', '0.7', '0.9', '0.8', '0.5', '0.7', '0.8', '0.9', '-0.8', '0.3', '0.95', '0.8', '0.8', '0.7', '0.2', '0.9', '0.8', '0.9', '0.5', '0.6', '0.8', '0.5', '0.8', '0.8', '0.8', '0.8', '-0.5', '0.6', '0.2', '0.8', '0.5', '0.8', '0.3', '0.9', '0.7', '0.8', '0.8', '0.7', '0.8', '0.8', '0.5', '0.7', '0.8', '0.8', '0.8', '0.8', '0.8', '0.8', '0.7', '0.9', '0.7', '0.9', '0.8', '0.8', '0.9', '0.2', '0.7', '0.5', '0.2', '0.3', '0.7', '0.7', '0.2', '0.6', '0.7', '0.7', '0.8', '0.7', '0.2', '0.8', '0.8', '0.8', '0.4', '0.7', '0.8', '0.9', '0.8', '0.9', '0.7', '0.85', '0.7', '0.6', '0.7', '0.95', '0.7', '0.5', '0.8', '0.8', '0.8', '0.8', '0.6', '0.7', '0.8', '0.9', '0.8', '0.5', '0.8', '0.7', '-0.36', '0.8', '0.95', '0.8', '0.4', '0.7', '0.8', '0.7', '0.45', '0.8', '0.7', '0.8', '0.8', '0.6', '0.9', '0.8', '0.7', '0.75', '0.3', '0.8', '0.8', '0.2', '0.3', '0.8', '0.5', '0.9', '0.9', '0.8', '0.8', '0.8', '0.7', '0.5', '0.8', '0.8', '0.7', '0.9', '0.7', '0.8', '0.6', '0.2', '0.9', '0.7', '0.7', '0.8', '0.8', '0.7', '0.7', '0.7', '0.95', '0.7', '0.8', '0.6', '0.6', '0.6', '0.8', '0.9', '0.7', '0.8', '0.85', '0.8', '0.95', '0.95', '0.9', '-0.6', '0.95', '0.9', '0.8', '0.8', '0.95', '0.8', '0.9', '0.9', '0.9', '0.9', '0.78', '0.8', '0.8', '0.8', '0.3', '0.8', '0.9', '0.2', '0.9', '0.6', '0.25', '0.5', '0.9', '0.2', '0.3', '0.9', '0.8', '0.8', '0.8', '0.8', '-0.75', '0.8', '0.95', '0.95', '0.95', '0.8', '0.9', '0.7', '0.8', '0.8', '0.7', '0.8', '0.9', '0.9', '0.9', '0.7', '0.8', '0.9', '0.9', '0.75', '0.8', '0.8', '0.8', '0.8', '0.3', '0.8', '-0.7', '0.7', '0.2', '0.2', '0.8', '0.8', '0.6', '0.8', '0.8', '0.7', '0.8', '0.8', '0.95', '0.8', '0.5', '0.8', '0.95', '0.9', '0.7', '0.8', '0.3', '0.8', '0.5', '0.9', '0.5', '0.9', '0.8', '0.7', '0.4', '0.7', '0.7', '0.8', '0.8', '0.6', '0.7', '0.9', '0.7', '0.6', '0.1', '0.6', '0.8', '0.9', '0.9', '0.85', '0.8', '0.6', '0.6', '0.9', '0.9', '0.95', '0.8', '0.95', '0.5', '0.9', '0.7', '-0.6', '0.7', '0.2', '0.7', '0.9', '0.95', '0.5', '0.9', '0.85', '0.7', '-0.7', '0.8', '0.6', '0.3', '0.5', '0.2', '0.7', '0.6', '0.6', '0.8', '0.2', '0.5', '0.7', '0.2', '0.8', '0.8', '0.7', '0.7', '-0.8', '0.8', '0.4', '0.8', '0.3', '0.2', '0.9', '0.7', '0.7', '0.7', '0.8', '0.7', '0.4', '0.8', '0.8', '-0.6', '0.8', '0.9', '0.5', '0.6', '0.8', '0.1', '0.6', '0.5', '0.8', '0.95', '-0.6', '0.8', '0.5', '0.7', '0.95', '0.95', '0.7', '0.9', '0.9', '0.6', '0.4', '0.95', '0.2', '0.4', '0.7', '-0.2', '0.8', '0.8', '0.7', '0.8', '0.8', '0.7', '0.6', '0.8', '0.9', '0.6', '0.8', '-0.7', '0.2', '0.3', '0.9', '0.9', '0.6', '0.9', '0.3', '0.6', '0.9', '0.3', '0.5', '0.6', '-0.8', '0.35', '0.2', '0.6', '0.2', '0.8', '0.3', '-0.4', '0.4', '0.8', '0.2', '0.7', '0.8', '0.9', '1', '0.8', '0.8', '0.9', '0.7', '0.9', '0.5', '0.7', '0.6', '0.7', '0.7', '0.8', '0.8', '0.7', '0.7', '0.15', '0.4', '-0.5', '0.5', '0.8', '0.4', '0.9', '0.9', '0.5', '0.4', '0.7', '0.7', '0.8', '0.8', '0.9', '0.9', '1', '0.8', '0.2', '0.8', '0.9', '0.2', '0.8', '0.6', '0.8', '0.8', '0.8', '0.7', '0.7', '0.8', '0.8', '0.8', '0.7', '0.9', '0.9', '0.7', '0.8', '0.9', '0.8', '0.7', '0.9', '0.7', '0.8', '0.7', '0.9', '0.3', '0.5', '0.8', '0.8', '0.3', '0.9', '0.6', '0.6', '0.4', '0.7', '0.6', '-0.6', '0.9', '0.6', '0.8', '0.7', '0.8', '0.8', '0.8', '0.2', '0.95', '0.8', '0.9', '0.8', '0.8', '0.95', '0.9', '0.9', '0.9', '0.7', '0.9', '0.8', '0.8', '0.9', '0.8', '0.8', '0.8', '0.85', '0.8', '0.9', '0.8', '0.95', '0.8', '0.9', '0.8', '0.95', '0.7', '0.8', '0.8', '0.6', '0.9', '0.8', '0.95', '0.8', '0.95', '0.4', '-0.8', '0.8', '0.8', '0.8', '0.9', '0.8', '0.8', '0.8', '0.8', '0.8', '0.7', '0.6', '0.95', '0.9', '0.3', '-0.7', '0.4', '0.4', '0.1', '0.7', '0.3', '0.8', '0.8', '0.8', '0.6', '0.7', '0.5', '0.75', '0.8', '0.7', '0.2', '0.8', '0.7', '0.7', '0.1', '0.7', '0.8', '0.8', '0.7', '0.7', '-0.5', '0.1', '0.9', '0.5', '0.6', '0.7', '0.7', '0.1', '0.1', '0.6', '0.4', '0.6', '0.9', '0.9', '0.8', '0.95', '0.85', '0.7', '0.4', '0.4', '0.7', '0.3', '0.8', '0.8', '0.7', '0.6', '0.8', '0.9', '0.65', '0.6', '0.6', '0.8', '0.1', '0.2', '0.7', '0.8', '0.9', '0.8', '0.7', '-0.43', '0.9', '0.8', '0.8', '0.7', '0.5', '0.9', '0.7', '0.9', '0.9', '0.7', '0.4', '0.6', '0.6', '0.9', '0.9', '0.6', '0.9', '0.8', '0.7', '0.7', '0.8', '0.9', '0.7', '0.9', '0.95', '0.8', '0.9', '0.9', '0.8', '0.9', '0.9', '0.9', '0.85', '0.8', '0.7', '0.8', '0.8', '0.95', '0.92', '0.71', '0.95', '0.8', '0.3', '0.1', '0.9', '0.7', '0.8', '0.8', '0.8', '0.9', '0.7', '0.7', '0.8', '0.8', '0.8', '0.8', '0.4', '0.8', '0.95', '0.9', '0.8', '0.2', '0.8', '0.5', '0.25', '0.9', '0.8', '0.7', '0.7', '0.7', '0.7', '0.3', '0.6', '0.8', '0.9', '0.9', '0.95', '0.7', '0.8', '0.8', '0.6', '0.95', '0.9', '0.6', '0.9', '0.2', '0.9', '0.8', '0.8', '0.8', '0.9', '0.8', '0.8', '0.7', '0.95', '0.9', '0.8', '0.7', '0.9', '0.8', '0.8', '0.95', '0.8', '0.8', '0.6', '0.7', '0.8', '0.8', '0.6', '0.7', '0.9', '0.2', '0.6', '0.95', '0.7', '0.6', '0.6', '0.9', '0.7', '0.95', '0.8', '0.8', '0.9', '0.95', '0.8', '0.8', '0.8', '1', '0.85', '0.6', '-0.7', '0.95', '0.9', '0.8', '0.8', '0.8', '0.8', '0.8', '0.9', '0.8', '0.9', '0.8', '0.5', '0.2', '0.8', '0.8', '0.9', '0.7', '0.5', '0.8', '0.8', '0.8', '0.9', '0.6', '0.7', '0.7', '0.7', '-0.85', '0.8', '0.95', '0.7', '0.9', '0.8', '0.8', '0.9', '0.7', '0.8', '0.8', '0.4', '0.8', '0.8', '0.9', '0.4', '0.7', '0.2', '0.8', '0.7', '0.6', '0.95', '0.8', '0.9', '0.7', '-0.6', '0.7', '0.8', '0.8', '0.95', '0.85', '-0.4', '0.8', '0.75', '0.95', '0.95', '0.9', '0.8', '0.8', '0.7', '0.8', '0.8', '0.2', '0.8', '0.8', '0.9', '0.7', '0.8', '0.9', '0.8', '0.8', '0.8', '0.8', '0.6', '0.8', '0.7', '0.95', '0.8', '0.4', '0.6', '0.7', '0.8', '0.1', '0.8', '0.7', '0.4', '0.8', '0.9', '0.8', '0.7', '0.9', '0.7', '0.8', '0.7', '0.8', '0.9', '0.6', '0.8', '0.7', '0.9', '0.9', '0.9', '0.8', '0.7', '0.8', '0.4', '0.7', '0.9', '0.8', '0.95', '0.8', '0.8', '0.8', '0.2', '0.8', '0.8', '0.8', '0.1', '0.7', '0.2', '0.9', '0.7', '0.2', '0.7', '0.35', '0.7', '0.75', '0.8', '0.8', '0.5', '0.3', '0.7', '0.8', '0.2', '0.8', '0.7', '0.7', '0.8', '0.7', '0.8', '0.8', '0.9', '0.5', '0.4', '0.7', '0.5', '0.7', '1', '0.8', '0.6', '0.6', '0.8', '0.8', '0.9', '0.8', '0.8', '0.5', '0.8', '0.7', '0.95', '0.4', '0.9', '0.8', '0.7', '0.8', '0.4', '0.6', '0.5', '0.2', '0.6', '0.92', '0.8', '0.8', '0.6', '0.6', '0.8', '0.6', '0.8', '0.9', '0.8', '0.8', '0.25', '0.7', '0.35', '0.6', '0.35', '0.5', '0.7', '0.7', '0.7', '0.8', '0.8', '0.8', '0.9', '0.7', '0.7', '0.8', '0.9', '0.8', '0.9', '0.8', '0.8', '0.8', '0.8', '0.7', '0.8', '0.3', '0.8', '0.8', '0.8', '0.8', '0.5', '0.95', '0.8', '0.7', '0.8', '0.9', '0.9', '0.8', '0.8', '0.8', '0.8', '0.2', '0.8', '0.3', '0.95', '0.9', '0.8', '0.8', '0.8', '0.6', '0.95', '0.6', '0.35', '0.9', '0.35', '0.7', '0.6', '0.9', '0.7', '1', '0.8', '0.9', '0.9', '0.8', '0.1', '0.7', '0.8', '0.8', '0.8', '0.6', '0.6', '0.9', '0.8', '0.7', '0.8', '0.8', '0.7', '0.5', '0.8', '0.8', '0.95', '0.8', '0.95', '0.9', '0.9', '0.9', '0.2', '0.8', '0.7', '0.8', '0.3', '0.7', '0.6', '0.3', '0.7', '0.8', '0.9', '0.9', '0.6', '0.3', '0.9', '0.7', '0.8', '0.5', '-0.6', '0.8', '0.2', '0.7', '0.9', '0.8', '0.5', '0.6', '0.7', '0.8', '0.9', '0.7', '0.5', '0.8', '0.8', '0.7', '0.5', '0.3', '0.7', '0.9', '0.8', '0.8', '0.8', '0.8', '0.7', '0.9', '0.8', '0.7', '0.2', '0.9', '0.9', '0.7', '0.8', '0.2', '0.7', '0.6', '0.7', '0.8', '0.9', '0.7', '0.7', '0.5', '0.9', '0.5', '0.7', '0.8', '0.3', '0.4', '0.8', '0.8', '0.95', '0.8', '0.6', '0.6', '0.8', '-0.2', '0.6', '0.8', '0.6', '0.85', '0.2', '0.7', '0.8', '0.8', '0.9', '0.9', '0.2', '0.7', '0.3', '0.5', '0.8', '0.4', '0.8', '0.8', '0.7', '0.6', '0.6', '0.85', '0.8', '0.6', '0.95', '0.7', '0.7', '0.9', '0.8', '0.1', '0.8', '0.8', '0.6', '0.8', '0.95', '0.6', '0.6', '0.8', '-0.5', '0.8', '0.8', '0.7', '0.8', '-0.7', '0.8', '0.8', '0.8', '0.6', '0.95', '0.4', '0.9', '0.3', '0.75', '0.7', '0.8', '0.7', '0.8', '0.8', '0.6', '0.8', '0.8', '0.7', '0.5', '0.9', '0.8', '0.95', '1', '0.7', '0.7', '0.7', '0.95', '0.8', '0.8', '0.8', '0.8', '0.6', '0.8', '0.8', '0.8', '0.8', '0.8', '0.8', '0.9', '0.85', '0.7', '0.5', '0.9', '0.7', '0.8', '0.9', '0.9', '0.95', '0.8', '0.9', '0.7', '0.75', '0.2', '0.8', '0.8', '0.9', '-0.3', '0.7', '0.8', '0.5', '0.7', '0.8', '0.3', '0.8', '0.7', '0.3', '0.9', '0.8', '0.9', '0.8', '0.9', '0.9', '0.95', '0.8', '0.7', '0.7', '0.9', '0.7', '0.8', '0.8', '0.9', '0.7', '0.8', '0.9', '0.7', '0.8', '0.8', '0.55', '0.3', '0.8', '0.8', '0.9', '0.8', '0.85', '0.4', '0.9', '0.8', '0.8', '0.8', '0.7', '0.7', '0.8', '0.4', '0.7', '0.7', '0.8', '0.7', '0.7', '0.8', '0.8', '0.2', '0.7', '0.9', '0.8', '0.7', '0.92', '0.95', '0.95', '-0.5', '0.9', '0.9', '0.15', '0.9', '0.8', '0.9', '0.8', '0.6', '0.7', '0.9', '0.7', '0.7', '0.6', '0.7', '0.6', '1', '0.8', '-0.38', '0.9', '0.8', '0.8', '0.7', '0.5', '0.8', '0.8', '0.8', '0.8', '0.8', '0.5', '0.7', '0.7', '0.2', '0.2', '0.8', '0.6', '0.9', '0.8', '0.8', '0.6', '0.8', '0.2', '0.8', '0.2', '0.8', '0.5', '0.3', '0.8', '0.7', '0.8', '0.6', '0.9', '0.3', '0.8', '0.8', '0.8', '0.8', '0.6', '0.9', '0.95', '0.5', '0.9', '0.4', '0.8', '0.8', '0.8', '0.3', '0.8', '0.8', '0.7', '-0.3', '-0.57', '0.9', '0.95', '0.5', '0.7', '0.5', '0.4', '0.9', '0.2', '0.9', '0.7', '0.7', '0.7', '0.8', '0.7', '0.7', '0.8', '0.8', '0.7', '0.8', '0.7', '0.7', '0.4', '0.6', '0.9', '0.8', '0.8', '0.6', '0.8', '0.9', '0.7', '0.4', '-0.2', '0.8', '0.25', '0.8', '0.95', '0.9', '0.8', '0.9', '0.8', '0.6', '0.3', '0.7', '0.3', '0.6', '0.95', '0.7', '0.6', '-0.3', '0.7', '0.8', '0.6', '0.2', '0.6', '0.2', '0.9', '0.95', '0.95', '0.8', '0.8', '0.95', '0.8', '0.95', '0.95', '0.8', '0.9', '0.95', '0.95', '0.95', '0.7', '0.8', '0.8', '0.8', '0.3', '0.8', '0.2', '0.7', '0.6', '0.8', '0.7', '0.2', '0.7', '0.4', '0.95', '0.8', '0.8', '0.7', '0.8', '0.8', '0.8', '0.8', '0.8', '0.8', '0.6', '0.9', '0.9', '0.9', '0.95', '0.7', '0.2', '0.6', '0.8', '0.3', '0.5', '0.8', '0.6', '0.6', '0.8', '0.9', '0.5', '0.9', '0.1', '-0.7', '0.9', '0.8', '0.6', '0.7', '0.8', '0.95', '0.8', '0.8', '0.7', '0.8', '0.8', '0.7', '0.9', '0.7', '0.9', '0.8', '0.8', '0.6', '0.3', '0.6', '0.7', '0.6', '0.7', '0.5', '0.9', '0.95', '0.8', '0.8', '0.5', '0.7', '-0.5', '0.8', '0.5', '0.8', '0.8', '0.7', '0.9', '0.8', '0.8', '0.9', '0.9', '0.9', '0.5', '0.9', '0.9', '0.8', '0.8', '0.4', '0.8', '0.7', '0.7', '0.8', '0.8', '0.95', '0.6', '0.8', '0.7', '0.15', '0.9', '0.8', '0.85', '0.7', '1', '0.4', '0.95', '0.9', '0.2', '0.7', '0.8', '0.7', '0.7', '1', '0.8', '0.6', '0.3', '0.7', '0.7', '0.5', '0.7', '0.8', '0.7', '0.8', '0.7', '0.8', '0.8', '0.5', '0.9', '0.8', '0.7', '0.8', '0.7', '0.9', '0.8', '0.9', '1', '0.95', '0.6', '0.8', '0.8', '0.9', '0.9', '0.95', '0.7', '0.8', '0.8', '0.7', '-0.7', '0.7', '0.5', '0.6', '0.95', '0.8', '0.9', '0.8', '0.9', '0.8', '0.8', '0.7', '0.9', '0.3', '0.9', '0.8', '0.9', '0.7', '0.6', '0.8', '0.8', '0.8', '0.8', '0.7', '0.8', '0.9', '0.2', '0.9', '0.8', '0.8', '0.6', '0.3', '0.75', '0.8', '0.8', '0.3', '0.95', '0.7', '0.95', '0.8', '0.9', '0.8', '0.8', '0.4', '0.8', '0.9', '0.8', '0.5', '0.9', '0.95', '0.9', '0.6', '0.65', '0.7', '0.5', '0.56', '0.2', '0.8', '0.8', '0.9', '0.6', '0.6', '0.8', '0.7', '0.95', '0.8', '0.8', '0.6', '0.8', '0.95', '0.5', '0.4', '0.9', '0.8', '0.7', '0.7', '1', '0.9', '0.2', '0.7', '0.9', '0.9', '0.7', '0.7', '0.7', '0.7', '0.45', '0.7', '0.7', '0.7', '0.7', '0.7', '0.85', '0.6', '0.8', '-0.7', '0.7', '0.8', '0.9', '0.8', '0.3', '0.8', '0.7', '0.7', '0.4', '0.7', '0.7', '0.9', '0.8', '0.8', '0.8', '0.8', '0.8', '0.3', '0.7', '0.6', '0.9', '0.5', '0.95', '0.3', '0.8', '0.8', '0.3', '0.7', '0.8', '0.5', '0.8', '0.6', '0.8', '0.7', '0.7', '0.8', '0.4', '0.7', '0.1', '0.8', '0.6', '0.7', '0.85', '0.7', '0.8', '0.9', '0.7', '0.5', '-0.7', '0.3', '0.9', '0.8', '0.7', '0.7', '0.3', '0.8', '0.95', '0.8', '0.2', '0.8', '0.7', '0.8', '0.25', '0.9', '0.95', '0.3', '0.8', '0.3', '0.1', '0.8', '0.2', '0.7', '0.8', '-0.5', '0.8', '0.8', '0.4', '0.3', '0.8', '0.8', '0.6', '0.4', '0.9', '0.6', '0.8', '0.3', '0.8', '0.8', '0.9', '0.5', '-0.5', '0.8', '0.8', '0.95', '0.4', '0.8', '0.3', '0.8', '0.5', '0.8', '0.6', '0.3', '0.9', '0.2', '0.85', '0.2', '0.2', '0.8', '0.3', '0.8', '0.65', '0.5', '0.8', '0.6', '0.9', '0.7', '0.2', '0.9', '0.7', '0.9', '0.5', '0.8', '0.7', '0.8', '0.8', '0.6', '0.3', '0.8', '0.8', '0.8', '0.6', '0.9', '0.3', '0.8', '0.6', '0.7', '0.7', '0.3', '0.5', '0.9', '0.7', '0.7', '0.95', '0.7', '0.6', '0.8', '0.8', '0.7', '0.8', '0.7', '0.3', '0.2', '0.8', '0.8', '0.3', '0.8', '0.2', '-0.65', '0.4', '0.8', '0.8', '0.6', '0.7', '0.6', '0.8', '0.4', '0.7', '1', '0.7', '0.4', '0.7', '0.8', '0.8', '0.8', '0.8', '0.8', '-0.6', '0.4', '0.4', '0.5', '0.3', '0.8', '0.6', '0.9', '0.1', '0.9', '0.85', '0.7', '0.3', '0.5', '0.6', '0.6', '0.8', '0.95', '0.8', '0.8', '0.6', '0.8', '0.95', '0.8', '0.9', '0.8', '0.7', '0.8', '0.8', '0.85', '0.7', '0.7', '0.8', '0.9', '0.9', '0.45', '0.9', '0.8', '0.3', '0.7', '0.6', '0.6', '0.8', '0.6', '0.9', '0.8', '0.7', '0.9', '0.8', '0.7', '0.5', '0.2', '0.8', '0.3', '0.9', '0.7', '0.7', '0.3', '0.9', '0.4', '0.8', '0.8', '0.8', '0.8', '0.9', '0.9', '0.8', '0.7', '0.8', '0.3', '0.3', '0.9', '0.1', '0.3', '0.1', '0.5', '0.8', '-0.6', '0.3', '0.5', '0,6', '0.7', '0.3', '0.8', '0.7', '0.7', '-0.8', '0.6', '0.35', '-0.4', '0.95', '0.7', '0.9', '0.9', '0.3', '0.8', '0.6', '0.4', '0.9', '0.8', '0.8', '0.9', '0.95', '0.8', '0.8', '0.8', '0.3', '0.8', '-0.9', '0.8', '0.9', '0.6', '0.7', '0.95', '0.9', '0.9', '0.8', '0.9', '-0.6', '0.85', '0.7', '0.8', '0.9', '0.5', '0.9', '0.8', '0.9', '0.7', '0.7', '-0.25', '0.9', '-0.3', '0.85', '0.9', '0.2', '0.8', '0.8', '0.9', '0.9', '0.9', '0.9', '0.8', '0.8', '0.8', '0.7', '0.8', '0.6', '0.7', '0.5', '0.8', '0.8', '0.95', '0.9', '0.8', '0.95', '0.9', '0.3', '0.5', '0.8', '-0.7', '0.6', '0.8', '0.8', '0.95', '0.95', '0.7', '0.95', '0.5', '0.9', '0.8', '1', '0.95', '0.4', '0.4', '-0.8', '0.7', '0.4', '0.4', '0.6', '0.6', '0.6', '0.8', '0.9', '0.9', '0.9', '0.7', '0.8', '0.7', '0.7', '0.6', '0.7', '0.8', '0.8', '0.6', '0.5', '0.6', '0.8', '0.9', '0.3', '0.4', '0.2', '0.9', '0.8', '0.3', '0.7', '0.8', '0.7', '0.9', '0.8', '0.7', '0.95', '0.95', '0.9', '0.2', '0.8', '0.7', '0.85', '0.6', '-0.5', '0.7', '0.8', '-0.3', '0.8', '0.8', '0.7', '0.7', '-0.6', '0.8', '0.8', '0.9', '0.5', '0.6', '0.9', '0.7', '0.2', '0.8', '0.8', '0.8', '0.8', '0.8', '-0.58', '0.6', '0.8', '0.7', '0.8', '0.8', '0.3', '0.7', '0.7', '0.75', '0.95', '0.9', '0.95', '0.9', '0.8', '0.8', '0.95', '0.9', '0.2', '0.9', '-0.6', '0.95', '0.8', '0.85', '0.8', '0.7', '0.7', '0.95', '0.2', '0.45', '0.7', '0.6', '0.7', '0.7', '0.8', '0.95', '1', '0.4', '0.6', '0.3', '0.7', '0.95', '0.45', '0.7', '0.8', '0.9', '0.9', '0.8', '0.7', '0.7', '0.6', '0.8', '0.6', '0.8', '0.8', '0.5', '0.7', '0.8', '0.3', '0.95', '0.6', '0.8', '0.9', '0.65', '0.8', '-0.4', '0.6', '0.8', '0.6', '0.9', '0.8', '0.8', '0.7', '0.7', '0.8', '0.8', '-0.3', '0.6', '0.1', '0.8', '0.7', '0.5', '0.9', '0.8', '0.3', '0.6', '0.5', '0.8', '0.3', '0.5', '0.8', '0.5', '0.8', '0.5', '0.5', '0.8', '0.8', '0.3', '0.9', '0.8', '0.7', '0.5', '0.7', '0.2', '0.6', '0.8', '0.8', '0.6', '0.9', '0.9', '0.7', '-0.5', '0.8', '0.8', '0.75', '0.8', '0.7', '0.7', '0.9', '0.6', '0.2', '0.1', '0.2', '0.6', '0.7', '0.9', '0.35', '0.8', '0.7', '0.8', '0.8', '0.7', '0.8', '0.75', '0.5', '0.6', '0.95', '0.8', '0.6', '-0.3', '-0.5', '0.8', '0.7', '0.3', '0.8', '0.6', '0.7', '0.1', '0.7', '0.2', '0.7', '0.7', '0.8', '0.8', '0.9', '0.7', '0.8', '0.1', '0.8', '0.3', '0.9', '0.8', '0.3', '0.8', '0.6', '-0.25', '0.8', '0.7', '0.4', '0.6', '0.85', '0.7', '0.7', '0.7', '0.8', '0.7', '0.8', '0.3', '0.8', '0.7', '0.9', '0.8', '0.95', '0.8', '0.9', '0.4', '0.6', '0.4', '0.95', '0.8', '0.6', '-0.3', '0.6', '0.9', '0.8', '0.5', '0.95', '0.8', '0.9', '0.6', '0.5', '0.6', '0.5', '0.6', '0.7', '0.75', '0.8', '0.9', '0.8', '0.7', '0.4', '0.5', '0.5', '0.9', '0.9', '0.8', '0.95', '0.9', '0.8', '0.95', '0.6', '0.7', '0.7', '0.7', '0.7', '0.8', '0.9', '0.9', '0.7', '0.7', '0.7', '0.5', '0.7', '0.8', '0.7', '0.6', '0.7', '0.8', '0.8', '0.8', '0.6', '0.95', '0.7', '0.8', '0.2', '0.8', '0.8', '0.7', '0.85', '0.5', '0.7', '-0.2', '0.95', '0.8', '0.7', '0.3', '0.7', '0.7', '0.6', '0.6', '0.5', '0.7', '0.7', '0.7', '0.5', '-0.8', '0.6', '0.9', '0.8', '0.7', '0.6', '0.7', '0.3', '0.8', '0.7', '0.7', '0.8', '0.8', '0.7', '0.3', '0.8', '0.8', '0.3', '0.3', '0.8', '0.8', '0.8', '0.9', '0.8', '0.6', '0.8', '0.7', '0.6', '0.3', '0.8', '0.4', '0.5', '0.9', '0.7', '0.7', '0.6', '0.8', '0.8', '0.8', '0.8', '0.8', '0.8', '0.75', '0.8', '0.9', '0.3', '0.6', '0.4', '0.8', '0.95', '0.7', '0.7', '0.7', '0.6', '0.7', '0.9', '0.8', '0.0', '0.8', '0.7', '0.8', '0.7', '0.4', '0.8', '0.8', '0.5', '0.8', '0.9', '0.85', '0.7', '0.3', '0.5', '0.6', '0.8', '0.8', '0.7', '-0.1', '0.85', '0.7', '0.8', '0.8', '0.8', '-0.95', '0.7', '0.95', '0.7', '0.2', '0.9', '0.6', '1', '0.3', '0.3', '0.25', '0.8', '0.8', '0.7', '0.9', '0.9', '0.9', '0.8', '0.4', '0.8', '0.3', '0.8', '0.5', '0.7', '0.7', '-0.8', '0.2', '0.7', '0.7', '0.8', '1', '0.1', '0.2', '0.95', '0.95', '0.95', '0.95', '1', '0.8', '0.8', '0.7', '0.9', '0.85', '0.6', '0.8', '0.8', '0.3', '-0.6', '0.4', '0.95', '0.8', '0.8', '0.5', '0.6', '0.8', '0.8', '0.7', '0.8', '0.8', '0.85', '0.9', '0.95', '0.85', '0.8', '0.8', '0.7', '0.8', '0.8', '0.8', '0.95', '-0.5', '0.8', '0.9', '0.6', '0.95', '0.1', '-0.85', '0.2', '0.8', '0.9', '0.7', '0.8', '0.8', '-0.6', '0.2', '0.7', '0.9', '1', '0.7', '0.6', '0.9', '0.8', '0.9', '0.9', '0.8', '-0.5', '0.95', '0.8', '0.8', '0.3', '0.9', '0.8', '0.8', '0.8', '0.9', '0.8', '0.8', '0.95', '0.8', '0.95', '0.8', '0.8', '0.9', '0.3', '0.9', '0.9', '0.8', '0.7', '1', '0.95', '0.95', '0.95', '0.9', '0.9', '0.8', '0.8', '0.8', '0.6', 'Sentiment Score: 0.3', '0.8', '0.7', '0.5', '0.98', '0.8', '0.9', '0.3', '0.9', '0.8', '0.6', '0.7', '0.9', '0.4', '0.6', '0.8', '0.7', '0.7', '0.1', '0.8', '0.8', '0.8', '0.9', '0.8', '0.6', '0.7', '0.8', '0.8', '0.6', '0.6', '0.7', '0.7', '0.2', '0.7', '0.8', '0.6', '0.8', '0.8', '0.8', '0.7', '0.6', '0.8', '0.4', '0.8', '0.4', '0.8', '0.3', '0.8', '0.2', '0.9', '0.6', '0.4', '0.9', '-0.6', '0.3', '0.8', '0.3', '0.8', '0.8', '0.8', '0.95', '0.2', '0.7', '0.7', '0.8', '0.7', '0.95', '0.8', '0.95', '0.9', '-0.5', '0.85', '0.7', '0.3', '0.8', '0.3', '0.2', '0.8', '0.5', '0.5', '0.8', '0.9', '0.6', '0.8', 'Sentiment Score: 0.25', '0.8', '0.2', '0.9', '0.9', '0.8', '0.85', '0.3', '-0.42', '0.3', '0.9', '0.8', '0.9', '0.2', '0.6', '0.9', '0.8', '0.9', '0.95', '0.8', '0.7', '0.7', '0.6', '0.9', '0.4', '0.8', '0.8', '0.8', '0.9', '0.2', '0.5', '0.7', '0.8', '0.9', '0.7', '0.7', '0.8', '0.8', '0.7', '0.8', '0.9', '0.8', '0.95', '0.9', '0.65', '0.8', '0.7', '0.8', '0.95', '0.9', '0.7', '0.7', '0.8', '0.8', '0.8', '0.8', '0.9', '0.4', '0.7', '0.85', '0.5', '0.5', '0.7', '0.7', '0.7', '0.7', '0.9', '-1', '0.6', '0.7', '0.5', '0.5', '0.8', '0.6', '0.3', '0.7', '0.7', '0.8', '-0.9', '0.9', '0.4', '0.7', '0.9', '0.5', '0.8', '0.9', '0.8', '0.7', '0.8', '0.8', '0.1', '0.8', '0.1', '0.2', '0.7', '0.7', '0.9', '0.8', '0.6', '0.95', '0.8', '0.8', '0.85', '0.5', '0.8', '0.7', '0.7', '0.3', '0.7', '0.9', '0.8', '0.6', '0.8', '0.4', '0.6', '0.7', '0.9', '0.8', '0.9', '0.5', '0.3', '0.5', '0.7', '0.8', '0.5', '0.7', '0.8', '0.7', '0.2', '0.5', '0.2', '0.8', '0.6', '0.9', '0.6', '0.7', '0.2', '0.8', '0.6', '0.8', '0.8', '0.8', '0.85', '0.3', '0.6', '0.9', '0.95', '0.9', '0.2', '0.85', '0.9', '0.3', '0.9', '0.8', '0.8', '0.8', '0.4', '0.8', '0.2', '-0.5', '-0.25', '0.7', '0.8', '0.9', '0.65', '0.2', '0.7', '0.9', '0.9', '0.9', '0.7', '0.8', '0.8', '0.6', '0.7', '0.7', '0.8', '0.7', '0.95', '0.95', '0.8', '0.3', '0.9', '0.7', '0.9', '0.4', '0.8', '0.8', '0.6', '0.8', '0.8', '0.8', '0.7', '0.8', '0.95', '0.9', '0.9', '0.9', '0.8', '0.7', '0.7', '0.7', '0.9', '0.8', '0.8', '0.8', '0.7', '0.7', '0.3', '0.8', '0.8', '0.85', '0.5', '0.6', '0.95', '0.9', '0.8', '0.8', '0.7', '0.7', '0.8', '0.7', '0.8', '0.9', '0.8', '0.8', '0.6', '0.8', '0.8', '0.8', '0.7', '0.8', '0.9', '0.9', '0.8', '0.8', '0.85', '0.8', '0.8', '0.8', '0.8', '0.8', '0.7', '0.9', '0.6', '0.7', '0.8', '0.75', '0.8', '0.8', '0.9', '0.6', '0.8', '0.7', '0.7', '0.9', '0.7', '0.9', '0.8', '0.7', '0.6', '0.95', '0.8', '0.2', '0.6', '0.4', '0.8', '0.4', '0.7', '0.9', '0.7', '0.7', '0.8', '0.8', '0.8', '0.8', '0.85', '0.8', '0.6', '0.85', '0.7', '0.7', '0.8', '0.9', '0.3', '0.3', '0.7', '0.7', '-0.6', '0.8', '0.9', '0.7', '0.6', '0.9', '0.7', '0.8', '0.8', '-0.5', '0.2', '0.4', '0.8', '0.4', '0.7', '0.5', '0.9', '0.2', '0.6', '0.5', '0.6', '0.6', '0.8', '0.8', '0.5', '0.5', '0.8', '0.8', '0.6', '0.8', '0.8', '0.9', '0.8', '0.8', '0.8', '0.8', '0.85', '0.7', '0.95', '0.6', '0.6', '0.4', '0.7', '0.7', '0.8', '0.7', '0.8', '0.6', '0.8', '0.8', '0.8', '0.7', '0.5', '0.4', '0.7', '0.8', '0.9', '0.4', '0.7', '0.5', '0.4', '0.6', '0.95', '0.8', '0.9', '0.9', '0.3', '0.8', '0.9', '0.9', '0.9', '0.4', '0.3', '0.7', '0.2', '0.7', '0.8', '0.95', '0.7', '0.9', '0.85', '0.4', '0.95', '0.4', '0.8', '0.7', '0.8', '0.6', '0.9', '0.9', '0.9', '0.8', '0.6', '0.95', '0.8', '0.8', '0.8', '0.8', '0.2', '0.8', '0.2', '0.5', '0.9', '0.7', '0.5', '0.9', '0.15', '0.6', '-0.3', '0.6', '0.8', '0.8', '0.9', '0.8', '0.9', '0.7', '0.8', '0.9', '0.7', '0.8', '0.9', '0.9', '0.8', '0.8', '0.95', '0.7', '0.3', '0.8', '0.8', '0.9', '0.9', '0.6', '0.7', '0.8', '0.8', '0.7', '0.9', '0.8', '0.5', '-0.5', '0.6', '0.8', '0.4', '0.8', '0.2', '0.7', '0.8', '0.8', '0.7', '0.8', '0.8', '0.7', '0.6', '0.3', '0.9', '0.8', '0.8', '0.8', '0.7', '0.9', '-0.7', '0.8', '0.9', '0.8', '0.8', '0.5', '0.7', '0.6', '0.7', '0.8', '0.8', '0.8', '0.6', '0.8', '0.7', '0.3', '0.8', '0.7', '0.6', '0.5', '0.5', '0.7', '0.6', '0.35', '0.8', '0.2', '0.8', '1', '0.8', '0.8', '0.7', '0.7', '0.7', '0.8', '0.8', '0.8', '0.8', '0.9', '0.9', '0.6', '0.8', '0.8', '0.8', '0.7', '-0.5', '0.6', '0.8', '0.8', '0.3', '0.8', '0.7', '0.9', '0.8', '0.25', '0.9', '0.2', '0.5', '0.9', '-0.7', '0.8', '0.8', '0.8', '0.4', '0.7', '-0.5', '0.0', '0.8', '0.7', '-0.8', '0.6', '-0.7', '0.8', '0.7', '0.8', '0.8', '0.95', '0.5', '0.3', '0.95', '0.8', '0.7', '0.8', '0.9', '0.7', '0.8', '0.72', '0.8', '0.8', '0.8', '0.3', '0.8', '0.8', '0.95', '0.9', '0.9', '0.5', '0.8', '0.8', '0.8', '0.9', '0.5', '0.3', '0.1', '0.7', '0.8', '0.95', '0.2', '0.8', '0.9', '0.95', '0.5', '0.3', '0.7', '-0.4', '0.9', '-0.35', '-0.5', '0.9', '0.7', '0.8', '0.6', '0.95', '0.7', '0.9', '0.8', '1', '0.6', '0.7', '0.9', '0.8', '0.7', '0.9', '0.5', '0.9', '0.3', '-0.9', '0.9', '0.8', '0.8', '0.7', '0.5', '0.7', '0.7', '0.76', '0.8', '0.9', '0.4', '0.8', '0.9', '0.4', '0.8', '0.8', '0.9', '0.6', '0.2', '0.8', '0.9', '0.6', '0.8', '0.8', '0.8', '0.8', '0.4', '0.85', '0.8', '0.8', '0.7', '0.5', '0.7', '0.8', '0.2', '0.8', '0.8', '0.85', '0.8', '0.7', '0.45', '0.95', '0.8', '0.8', '0.8', '0.8', '0.8', '0.8', '0.5', '0.7', '0.7', '0.7', '0.9', '0.8', '0.6', '0.5', '0.5', '0.8', '0.8', '0.7', '0.8', '0.8', '0.9', '0.8', '0.8', '0.8', '0.9', '0.8', '0.7', '0.9', '0.7', '0.9', '0.6', '0.5', '0.9', '0.8', '0.8', '0.8', '0.8', '0.8', '0.6', '0.95', '0.4', '0.3', '0.9', '0.6', '0.6', '0.7', '0.9', '0.8', '0.5', '0.9', '0.8', '0.7', '0.7', '0.8', '0.1', '0.7', '0.5', '0.8', '0.9', '0.6', '0.7', '0.6', '0.8', '0.8', '0.8', '0.5', '0.45', '0.6', '0.4', '0.65', '0.8', '0.8', '0.7', '0.65', '0.8', '0.5', '0.8', '0.8', '0.5', '0.2', '0.2', '0.6', '0.5', '0.9', '0.77', '0.6', '0.7', '0.8', '0.1', '0.5', '0.6', '0.15', '0.8', '0.7', '0.8', '0.8', '0.3', '0.9', '0.3', '0.7', '0.8', '0.2', '0.7', '0.6', '0.9', '0.6', '0.5', '0.9', '0.6', '0.8', '0.8', '0.7', '0.2', '0.6', '0.9', '0.7', '0.8', '0.6', '0.8', '0.9', '-0.7', '0.7', '0.8', '0.8', '0.7', '0.8', '-0.4', '0.8', '0.8', '0.95', '0.7', '0.5', '0.95', '0.7', '0.4', '0.9', '0.8', '0.8', '0.7', '0.3', '0.9', '0.6', '0.5', '0.5', '0.8', '0.8', '0.5', '0.7', '0.8', '0.8', '0.8', '0.7', '0.7', '0.2', '0.8', '0.8', '0.95', '0.8', '0.7', '0.8', '0.7', '0.8', '0.6', '0.9', '0.8', '0.8', '0.8', '0.9', '0.9', '0.8', '0.8', '0.8', '0.8', '0.95']\n"
     ]
    }
   ],
   "source": [
    "data_subset = data[messagecolumn]\n",
    "\n",
    "# Run sentiment analysis in parallel with 5 threads\n",
    "response = process_in_parallel(data_subset, max_workers=5)\n",
    "\n",
    "# The result is a list of responses\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Product Review</th>\n",
       "      <th>Cosine Similarity</th>\n",
       "      <th>Cosine Similarity TFIDF</th>\n",
       "      <th>Sentiment Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...</td>\n",
       "      <td>You need personal informations from companies,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...</td>\n",
       "      <td>Bottle after MBCC 2024. Black colour, malty ar...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...</td>\n",
       "      <td>Thank you for sharing this Chris - Black with ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...</td>\n",
       "      <td>Boxed beer at home, proper glassware. Pitch bl...</td>\n",
       "      <td>0.187317</td>\n",
       "      <td>0.134447</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...</td>\n",
       "      <td>From backlog. (As 2018 Vintage) 0,3 litre Bott...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8226</th>\n",
       "      <td>Superstition Blue Berry White🇺🇸Mead - Melomel ...</td>\n",
       "      <td>Bottle 82/100. Pours a deep, inky purple. More...</td>\n",
       "      <td>0.102062</td>\n",
       "      <td>0.059846</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8227</th>\n",
       "      <td>Superstition Blue Berry White🇺🇸Mead - Melomel ...</td>\n",
       "      <td>Pours a deep berry hue. Aroma of rich blueberr...</td>\n",
       "      <td>0.154303</td>\n",
       "      <td>0.091090</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8228</th>\n",
       "      <td>Superstition Blue Berry White🇺🇸Mead - Melomel ...</td>\n",
       "      <td>If you look at my reviews, you will see how hi...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8229</th>\n",
       "      <td>Superstition Blue Berry White🇺🇸Mead - Melomel ...</td>\n",
       "      <td>Huge thank you to Dakine for sharing this with...</td>\n",
       "      <td>0.121268</td>\n",
       "      <td>0.071261</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8230</th>\n",
       "      <td>Superstition Blue Berry White🇺🇸Mead - Melomel ...</td>\n",
       "      <td>Deep red/purple. Oh my. The berries, white cho...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8231 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Product Name                                     Product Review  Cosine Similarity  Cosine Similarity TFIDF Sentiment Scores\n",
       "0     Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...  You need personal informations from companies,...           0.000000                 0.000000             -0.7\n",
       "1     Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...  Bottle after MBCC 2024. Black colour, malty ar...           0.000000                 0.000000              0.8\n",
       "2     Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...  Thank you for sharing this Chris - Black with ...           0.000000                 0.000000              0.7\n",
       "3     Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...  Boxed beer at home, proper glassware. Pitch bl...           0.187317                 0.134447              0.9\n",
       "4     Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...  From backlog. (As 2018 Vintage) 0,3 litre Bott...           0.000000                 0.000000              0.8\n",
       "...                                                 ...                                                ...                ...                      ...              ...\n",
       "8226  Superstition Blue Berry White🇺🇸Mead - Melomel ...  Bottle 82/100. Pours a deep, inky purple. More...           0.102062                 0.059846              0.8\n",
       "8227  Superstition Blue Berry White🇺🇸Mead - Melomel ...  Pours a deep berry hue. Aroma of rich blueberr...           0.154303                 0.091090              0.8\n",
       "8228  Superstition Blue Berry White🇺🇸Mead - Melomel ...  If you look at my reviews, you will see how hi...           0.000000                 0.000000              0.8\n",
       "8229  Superstition Blue Berry White🇺🇸Mead - Melomel ...  Huge thank you to Dakine for sharing this with...           0.121268                 0.071261              0.8\n",
       "8230  Superstition Blue Berry White🇺🇸Mead - Melomel ...  Deep red/purple. Oh my. The berries, white cho...           0.000000                 0.000000             0.95\n",
       "\n",
       "[8231 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Sentiment Scores'] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to csv\n",
    "data.to_csv(outputfile, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task E\n",
    "\n",
    "Create an evaluation score for each beer that uses both similarity and sentiment scores. \n",
    "Now recommend 3 products to the customer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input File\n",
    "inputfile = 'CDE.bagofwords.csv'\n",
    "\n",
    "# Output File\n",
    "outputfile = 'CDE.bagofwords.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Evaluation Score Function\n",
    "def evaluation(cosine_sim, sentiment):\n",
    "    try:\n",
    "        norm_sentiment = (float(sentiment) + 1) / 2\n",
    "        return ((cosine_sim * 0.8) + (norm_sentiment * 0.2))\n",
    "    except Exception as e:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "data = pd.read_csv(inputfile)\n",
    "cosine_sim = data['Cosine Similarity TFIDF']\n",
    "sentiment = data['Sentiment Scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of Evaluation Score\n",
    "evaluation_list = []\n",
    "for i in range(len(data)):\n",
    "    evaluation_list.append(evaluation(cosine_sim[i], sentiment[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Evaluation Score \n",
    "data['Evaluation Score'] = evaluation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning Sentiment into Floats\n",
    "sentiment = []\n",
    "for i in data['Sentiment Scores']:\n",
    "    try:\n",
    "            sentiment.append(float(i))\n",
    "    except Exception as e:\n",
    "            sentiment.append(0)\n",
    "data['Sentiment Scores'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Evaluation Scores per Beer\n",
    "beers = data['Product Name'].drop_duplicates()\n",
    "beer_score = data[['Product Name', 'Evaluation Score', 'Cosine Similarity TFIDF', 'Sentiment Scores']].groupby('Product Name').mean('Evaluation Score')\n",
    "beer_score = beer_score.sort_values('Evaluation Score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to CSV\n",
    "data.to_csv(outputfile)\n",
    "\n",
    "beer_score.to_csv('E.beer_recommendations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Evaluation Score</th>\n",
       "      <th>Cosine Similarity TFIDF</th>\n",
       "      <th>Sentiment Scores</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Product Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Superstition Grand Cru Berry - F.O. Barrel Aged🇺🇸Mead - Melomel / Fruited</th>\n",
       "      <td>0.202101</td>\n",
       "      <td>0.034167</td>\n",
       "      <td>0.747674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B. Nektar Ken Schramm Signature Series - The Heart of Darkness🇺🇸Mead - Melomel / Fruited</th>\n",
       "      <td>0.200601</td>\n",
       "      <td>0.024627</td>\n",
       "      <td>0.809000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Toppling Goliath SR-71 Blackbird (2015 Bottling / Draft)🇺🇸Stout - Imperial</th>\n",
       "      <td>0.198969</td>\n",
       "      <td>0.037700</td>\n",
       "      <td>0.688090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Evaluation Score  Cosine Similarity TFIDF  Sentiment Scores\n",
       "Product Name                                                                                                   \n",
       "Superstition Grand Cru Berry - F.O. Barrel Aged...          0.202101                 0.034167          0.747674\n",
       "B. Nektar Ken Schramm Signature Series - The He...          0.200601                 0.024627          0.809000\n",
       "Toppling Goliath SR-71 Blackbird (2015 Bottling...          0.198969                 0.037700          0.688090"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recommendation\n",
    "beer_score[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results \n",
    "$$\n",
    "\\text{Evaluation Score} = 0.8 * \\text{Similarity} + 0.2 * \\text{Sentiment}\n",
    "$$\n",
    "\n",
    "# EVAd PUT THING HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task F \n",
    "\n",
    "How would your recommendations change if you use word vectors (e.g., the spaCy package with medium sized pretrained word vectors) instead of the plain vanilla bag-of-words cosine similarity? One way to analyze the difference would be to consider the % of reviews that mention a preferred attribute. E.g., if you recommend a product, what % of its reviews mention an attribute specified by the customer? Do you see any difference across bag-of-words and word vector approaches? Explain. This article may be useful: https://medium.com/swlh/word-embeddings-versus-bag-of-words-the-curious-case-of-recommender-systems-6ac1604d4424?source=friends_link&sk=d746da9f094d1222a35519387afc6338\n",
    "\n",
    "\n",
    "Note that the article doesn’t claim that bag-of-words will always be better than word embeddings for recommender systems. It lays out conditions under which it is likely to be the case. That is, depending on the attributes you use, you may or may not see the same effect. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input File Name\n",
    "inputfile = 'CDE.bagofwords.csv'\n",
    "\n",
    "# Output File Name\n",
    "outputfile = 'CDE.bagofwords.csv'\n",
    "\n",
    "# Translated Column Messages\n",
    "messagecolumn = 'Product Review'\n",
    "\n",
    "# Import Data\n",
    "data = pd.read_csv(inputfile)\n",
    "\n",
    "# User Attributes\n",
    "userattributes = ['thick', 'rich', 'bodied']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the Attributes\n",
    "text1 = ' '.join(userattributes)\n",
    "doc1 = nlp(text1)\n",
    "\n",
    "# Initializing the Lists\n",
    "spacy_scores = []\n",
    "\n",
    "# Calculating Similarity\n",
    "for text2 in data[messagecolumn]:\n",
    "    doc2 = nlp(text2)\n",
    "    spacy_scores.append(doc1.similarity(doc2))\n",
    "\n",
    "# Saving Results to DataFrame\n",
    "data['Spacy Similarity'] = spacy_scores\n",
    "\n",
    "# Outputting CSV\n",
    "data.to_csv(outputfile, index = False)\n",
    "\n",
    "# Inputting Recommendations\n",
    "recommendations = pd.read_csv(\"E.beer_recommendations.csv\")\n",
    "recommendation = recommendations['Product Name'][0:3].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent of Each Beer That Contains the Attributes\n",
    "percent_TFIDF = []\n",
    "percent_spacy = []\n",
    "\n",
    "for name in recommendation:\n",
    "    total_count = int(data[data['Product Name'] == name]['Product Name'].count())\n",
    "    non_zero_TFIDF = int((data[data['Product Name'] == name]['Cosine Similarity TFIDF'] != 0).sum())\n",
    "    non_zero_spacy = int((data[data['Product Name'] == name]['Spacy Similarity'] != 0).sum())\n",
    "    percent_TFIDF.append((non_zero_TFIDF / total_count) * 100)\n",
    "    percent_spacy.append((non_zero_spacy / total_count) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Bag of Words Similarity</th>\n",
       "      <th>Vector Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Superstition Grand Cru Berry - F.O. Barrel Age...</td>\n",
       "      <td>27.906977</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B. Nektar Ken Schramm Signature Series - The H...</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Toppling Goliath SR-71 Blackbird (2015 Bottlin...</td>\n",
       "      <td>42.696629</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name  Bag of Words Similarity  Vector Similarity\n",
       "0  Superstition Grand Cru Berry - F.O. Barrel Age...                27.906977              100.0\n",
       "1  B. Nektar Ken Schramm Signature Series - The H...                28.000000              100.0\n",
       "2  Toppling Goliath SR-71 Blackbird (2015 Bottlin...                42.696629              100.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing Results \n",
    "comparison = pd.DataFrame()\n",
    "comparison['Product Name'] = recommendation\n",
    "comparison['Bag of Words Similarity'] = percent_TFIDF\n",
    "comparison['Vector Similarity'] = percent_spacy\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding New Evaluation Scores Using Spacy\n",
    "# Calculate Evaluation Score Function\n",
    "def evaluation(cosine_sim, sentiment):\n",
    "    try:\n",
    "        norm_sentiment = (float(sentiment) + 1) / 2\n",
    "        return ((cosine_sim * 0.8) + (norm_sentiment * 0.2))\n",
    "    except Exception as e:\n",
    "        return(0)\n",
    "# Import Data\n",
    "cosine_sim = data['Spacy Similarity']\n",
    "sentiment = data['Sentiment Scores']\n",
    "\n",
    "# Calculation Evaluation Score\n",
    "evaluation_list = []\n",
    "for i in range(len(data)):\n",
    "    evaluation_list.append(evaluation(cosine_sim[i], sentiment[i]))\n",
    "\n",
    "# Save Evaluation Score \n",
    "data['Spacy Evaluation Score'] = evaluation_list\n",
    "\n",
    "# Aggregate Evaluation Scores per Beer\n",
    "beers = data['Product Name'].drop_duplicates()\n",
    "beer_score = data[['Product Name', 'Spacy Evaluation Score', 'Spacy Similarity', 'Sentiment Scores']].groupby('Product Name').mean('Spacy Evaluation Score')\n",
    "beer_score = beer_score.sort_values('Spacy Evaluation Score', ascending = False)\n",
    "\n",
    "# Output to CSV\n",
    "data.to_csv(outputfile, index = False)\n",
    "\n",
    "beer_score.to_csv('F.new_recommendations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spacy Evaluation Score</th>\n",
       "      <th>Spacy Similarity</th>\n",
       "      <th>Sentiment Scores</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Product Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Anchorage A Deal With The Devil - Double Oaked 2017🇺🇸Barley Wine / Wheat Wine / Rye Wine</th>\n",
       "      <td>0.634633</td>\n",
       "      <td>0.581589</td>\n",
       "      <td>0.693617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sahtipaja MeadMe Batch #2 - Bourbon Vanilla🇸🇪Mead - Melomel / Fruited</th>\n",
       "      <td>0.625957</td>\n",
       "      <td>0.564998</td>\n",
       "      <td>0.739583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Superstition Grand Cru Berry - F.O. Barrel Aged🇺🇸Mead - Melomel / Fruited</th>\n",
       "      <td>0.612530</td>\n",
       "      <td>0.547203</td>\n",
       "      <td>0.747674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Spacy Evaluation Score  Spacy Similarity  Sentiment Scores\n",
       "Product Name                                                                                                  \n",
       "Anchorage A Deal With The Devil - Double Oaked ...                0.634633          0.581589          0.693617\n",
       "Sahtipaja MeadMe Batch #2 - Bourbon Vanilla🇸🇪Me...                0.625957          0.564998          0.739583\n",
       "Superstition Grand Cru Berry - F.O. Barrel Aged...                0.612530          0.547203          0.747674"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New Recommendations\n",
    "beer_score[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "There is a clear difference between bag of words and spacy. It seems that spacy inflates the cosine similarity higher than the bag of words model. That's likely because spacy allows similar words to count towards the similarity evaluation whereas cosine similarity requires a one to one match between words to count a similarity. However, in a niche context such as attributes of beer, the safer route is to use bag of words because the words in this context have every specific meaning, and accepting other meanings of the word is not beneficial. We can see that the beers recommended completely changed from the bag of words model. We also see that spacy has found some sort of similarity between every single review of every beer to the attributes described by the user. \n",
    "\n",
    "In this scenario, word embeddings is not helpful. Looking through the data, there are some messages that are spam messages, having no similarity with beers or the attributes described by the user, yet spacy assigned a significant score to the spam message. Therefore, in this context, similarity is inflating similarity scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You need personal informations from companies,family and friends that will better your life and you need easy access without them noticing\n",
      "just contact nick or you’re financially unstable or you have a bad credit score, he will solve that without stress,he and his team can clear\n",
      "criminal records without leaving a trace and can also anonymously credit your empty credit cards with funds you need,all these are not done\n",
      "free obviously but I like working with nick and his team cause they keep you updated on every step taken in order to achieve the goal and\n",
      "they also deliver on time,I tested and confirmed this I’m still happy on how my life is improving after my encounter with him ,you can send\n",
      "a mail to Premiumhackservices AT gmail DOT com, Whatsapp: +14106350697.\n",
      "\n",
      "The calculated spacy similarity was: 0.18\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "data = pd.read_csv('CDE.bagofwords.csv')\n",
    "\n",
    "data[['Product Review', 'Spacy Similarity']].head(1)\n",
    "\n",
    "print(textwrap.fill(data['Product Review'][0], width = 140))\n",
    "result = data['Spacy Similarity'][0]\n",
    "print(f'\\nThe calculated spacy similarity was: {result:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task G\n",
    "\n",
    "How would your recommendations differ if you ignored the similarity and feature sentiment scores and simply chose the 3 highest rated products from your entire dataset? Would these products meet the requirements of the user looking for recommendations? Why or why not? Justify your answer with analysis. Use the similarity and sentiment scores as well as overall ratings to answer this question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input File Names\n",
    "inputfile = 'CDE.bagofwords.csv'\n",
    "inputfile2 = 'translated_messages.csv'\n",
    "\n",
    "# Translated Column Messages\n",
    "messagecolumn = 'Product Review'\n",
    "ratingcolumn = 'rating'\n",
    "\n",
    "# Import Data\n",
    "data = pd.read_csv(inputfile)\n",
    "data2 = pd.read_csv(inputfile2)\n",
    "\n",
    "# Turning Sentiment into Floats\n",
    "sentiment = []\n",
    "for i in data['Sentiment Scores']:\n",
    "    try:\n",
    "            sentiment.append(float(i))\n",
    "    except Exception as e:\n",
    "            sentiment.append(0)\n",
    "data['Sentiment Scores'] = sentiment\n",
    "\n",
    "# Find Top 3 Highest Rated Beers\n",
    "data['Ratings'] = data2[ratingcolumn]\n",
    "highest_rated = data[['Product Name', 'Evaluation Score', 'Cosine Similarity TFIDF', 'Sentiment Scores', 'Ratings']].groupby('Product Name').mean(['Evaluation Score','Ratings']).sort_values('Ratings', ascending=False)[0:3]\n",
    "recommendations = data[['Product Name', 'Evaluation Score', 'Cosine Similarity TFIDF', 'Sentiment Scores', 'Ratings']].groupby('Product Name').mean(['Evaluation Score','Ratings']).sort_values('Evaluation Score', ascending=False)[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on ratings alone:\n",
      "                                                    Evaluation Score  Cosine Similarity TFIDF  Sentiment Scores   Ratings\n",
      "Product Name                                                                                                             \n",
      "Toppling Goliath Kentucky Brunch🇺🇸Stout - Imper...          0.192930                 0.023483          0.741436  4.563536\n",
      "Cigar City Pilot Series Miami Madness🇺🇸Berliner...          0.189680                 0.006743          0.842857  4.561905\n",
      "Side Project Beer : Barrel : Time - 2018🇺🇸Stout...          0.186155                 0.015031          0.741304  4.560870\n",
      "\n",
      "\n",
      "Based on bag of words similarity and sentiment:\n",
      "                                                    Evaluation Score  Cosine Similarity TFIDF  Sentiment Scores   Ratings\n",
      "Product Name                                                                                                             \n",
      "Superstition Grand Cru Berry - F.O. Barrel Aged...          0.202101                 0.034167          0.747674  4.439535\n",
      "B. Nektar Ken Schramm Signature Series - The He...          0.200601                 0.024627          0.809000  4.548000\n",
      "Toppling Goliath SR-71 Blackbird (2015 Bottling...          0.198969                 0.037700          0.688090  4.355056\n"
     ]
    }
   ],
   "source": [
    "# Compare \n",
    "pd.set_option('display.expand_frame_repr', False)  # Do not wrap the output\n",
    "print(\"Based on ratings alone:\")\n",
    "print(highest_rated)\n",
    "print(\"\\n\\nBased on bag of words similarity and sentiment:\")\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "The results would not satisfy the user because these beers do not possess the attributes the consumer is looking for. This is reflected in the higher rated beers having lower evaluation scores which combines sentiment and cosine similarity. In addition, the recommended beers still have high ratings compared to the highest rated, and so the quality of the beer is surely still acceptable and more fitting to the user's taste. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task H\n",
    "\n",
    "Choose any 10 beers in your data. Now choose any one of them, and find the most similar beer (among the remaining 9). Explain your method and logic. https://medium.datadriveninvestor.com/who-is-your-competitor-in-the-era-of-the-long-tail-d0ac24fedde8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input File Names\n",
    "inputfile = 'CDE.bagofwords.csv'\n",
    "inputfile2 = 'translated_messages.csv'\n",
    "\n",
    "# Import Data\n",
    "data = pd.read_csv(inputfile)\n",
    "data2 = pd.read_csv(inputfile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Top 10 Beers\n",
    "data['Ratings'] = data2['rating']\n",
    "#highest_rated = \n",
    "highest_rated = data[['Product Name', 'Ratings']].groupby('Product Name').mean('Ratings').sort_values('Ratings', ascending=False).index[0:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent Each Beer As a Single Document\n",
    "beer_messages = pd.DataFrame()\n",
    "beer_messages['Beer'] = highest_rated\n",
    "\n",
    "fullmessages = []\n",
    "for beer in highest_rated:\n",
    "    fullmessages.append(data[data['Product Name'] == beer]['Product Review'].str.cat(sep = ' '))\n",
    "\n",
    "beer_messages['Combined Reviews'] = fullmessages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Cosine Similarity Between Highest Rated Beer and Rest of Top 10 Using Bag of Words Model\n",
    "\n",
    "# Setting Target Beer\n",
    "text1 = beer_messages['Combined Reviews'][0]\n",
    "\n",
    "# Initializing the Lists\n",
    "similarity_scores = []\n",
    "similarity_scorestfidf = []\n",
    "\n",
    "# Calculating Similarity\n",
    "for text2 in beer_messages['Combined Reviews'][1:10]:\n",
    "    documents =[text1, text2]\n",
    "    \n",
    "    # Non-Normalized\n",
    "    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "    sparse_matrix = count_vectorizer.fit_transform(documents)\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    df = pd.DataFrame(doc_term_matrix,\n",
    "        columns=count_vectorizer.get_feature_names_out(),\n",
    "        index=['x', 'y'])\n",
    "    similarity_scores.append(cosine_similarity(df, df)[0,1])\n",
    "\n",
    "    # Normalized\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "    sparse_matrixtfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "    doc_term_matrixtfidf = sparse_matrixtfidf.todense()\n",
    "    dftfidf = pd.DataFrame(doc_term_matrixtfidf, \n",
    "        columns = tfidf_vectorizer.get_feature_names_out(),\n",
    "        index = ['x', 'y'])\n",
    "    similarity_scorestfidf.append(cosine_similarity(dftfidf, dftfidf)[0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beer 1</th>\n",
       "      <th>Beer 2</th>\n",
       "      <th>Cosine Similarity</th>\n",
       "      <th>Normalized Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...</td>\n",
       "      <td>Goose Island Bourbon County Stout - Rare 2010🇺...</td>\n",
       "      <td>0.842742</td>\n",
       "      <td>0.835625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...</td>\n",
       "      <td>Side Project Beer : Barrel : Time - 2018🇺🇸Stou...</td>\n",
       "      <td>0.774314</td>\n",
       "      <td>0.715586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...</td>\n",
       "      <td>Cycle / 3 Sons Rare Scooop🇺🇸Stout - Imperial F...</td>\n",
       "      <td>0.652199</td>\n",
       "      <td>0.567901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...</td>\n",
       "      <td>Superstition Grand Cru Berry - F.O. Barrel Age...</td>\n",
       "      <td>0.485763</td>\n",
       "      <td>0.440068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...</td>\n",
       "      <td>Sahtipaja MeadMe Batch #2 - Bourbon Vanilla🇸🇪M...</td>\n",
       "      <td>0.478520</td>\n",
       "      <td>0.408192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...</td>\n",
       "      <td>B. Nektar Ken Schramm Signature Series - The H...</td>\n",
       "      <td>0.474992</td>\n",
       "      <td>0.380317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...</td>\n",
       "      <td>Schramm's The Heart of Darkness🇺🇸Mead - Melome...</td>\n",
       "      <td>0.471103</td>\n",
       "      <td>0.375422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...</td>\n",
       "      <td>Cigar City Pilot Series Dragonfruit Passion Fr...</td>\n",
       "      <td>0.318535</td>\n",
       "      <td>0.231672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...</td>\n",
       "      <td>Cigar City Pilot Series Miami Madness🇺🇸Berline...</td>\n",
       "      <td>0.276189</td>\n",
       "      <td>0.187509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Beer 1  \\\n",
       "7  Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...   \n",
       "1  Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...   \n",
       "5  Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...   \n",
       "8  Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...   \n",
       "4  Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...   \n",
       "3  Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...   \n",
       "6  Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...   \n",
       "2  Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...   \n",
       "0  Toppling Goliath Kentucky Brunch🇺🇸Stout - Impe...   \n",
       "\n",
       "                                              Beer 2  Cosine Similarity  \\\n",
       "7  Goose Island Bourbon County Stout - Rare 2010🇺...           0.842742   \n",
       "1  Side Project Beer : Barrel : Time - 2018🇺🇸Stou...           0.774314   \n",
       "5  Cycle / 3 Sons Rare Scooop🇺🇸Stout - Imperial F...           0.652199   \n",
       "8  Superstition Grand Cru Berry - F.O. Barrel Age...           0.485763   \n",
       "4  Sahtipaja MeadMe Batch #2 - Bourbon Vanilla🇸🇪M...           0.478520   \n",
       "3  B. Nektar Ken Schramm Signature Series - The H...           0.474992   \n",
       "6  Schramm's The Heart of Darkness🇺🇸Mead - Melome...           0.471103   \n",
       "2  Cigar City Pilot Series Dragonfruit Passion Fr...           0.318535   \n",
       "0  Cigar City Pilot Series Miami Madness🇺🇸Berline...           0.276189   \n",
       "\n",
       "   Normalized Similarity  \n",
       "7               0.835625  \n",
       "1               0.715586  \n",
       "5               0.567901  \n",
       "8               0.440068  \n",
       "4               0.408192  \n",
       "3               0.380317  \n",
       "6               0.375422  \n",
       "2               0.231672  \n",
       "0               0.187509  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Saving to Dataframe\n",
    "beersimilarities = pd.DataFrame()\n",
    "beersimilarities['Beer 1'] = highest_rated[0:1] * (len(highest_rated) - 1)\n",
    "beersimilarities['Beer 2'] = highest_rated[1:10]\n",
    "beersimilarities['Cosine Similarity'] = similarity_scores\n",
    "beersimilarities['Normalized Similarity'] = similarity_scorestfidf\n",
    "\n",
    "beersimilarities.sort_values('Cosine Similarity', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method & Logic\n",
    "I chose the top 10 rated beers for this analysis. I made the target beer the number one highest rated beer by rating and found the cosine similarity with the rest of the top 10. \n",
    "\n",
    "For the similarity calculations, I took every review of each beer and joined it into 1 long review. This made it easy to compare reviews using bag of words cosine similarity. \n",
    "\n",
    "I chose bag of words cosine similarity because as we saw before, the spacy similarity was not appropritate for this scenario, inflating similarities greatly. \n",
    "\n",
    "I used both count vectorizer and tfidf vectorizer to find cosine similarities to account for frequency of word usage among reviews, this was done because I did not account for stop words in my analysis. \n",
    "\n",
    "You'll find that if you use the count or tfidf vectorizer, you end up with similar results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
