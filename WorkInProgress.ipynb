{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Crowdsourced Recommender System (WORKING FILE)\n",
    "\n",
    "### Group Members: Jose Currea, Jenna Ferguson, Evan Hadd, Ramzi Kattan, Hadley Krummel, Jennifer Gonzalez, Ibrahim Muhammad\n",
    "### Class Section: Afternoon 1 - 3pm\n",
    "\n",
    "It should accept user inputs in the form of desired attributes of a product and come up with 3 recommendations. \n",
    "\n",
    "**Your Python Notebook should include the following:**\n",
    "- All scripts \n",
    "- The sentiment and similarity scores for the three products you recommended in task E.\n",
    "- Your analyses for and answer to task F. Make sure you show the ratings, similarity scores and sentiments for the products you recommend in tasks E and F. Use tables whenever possible.  \n",
    "- Show the logic you are using in addition to finding the most similar product. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import Counter\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "import itertools\n",
    "from sklearn.manifold import MDS\n",
    "import statsmodels.api as sm  # For the OLS regression\n",
    "import numpy as np            # For numerical operations like log transformations\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "from collections import Counter  # For counting word occurrences\n",
    "from scipy import stats        # For t-statistic and p-value calculations\n",
    "from sklearn import manifold\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.expand_frame_repr', False)  # Prevent wrapping to multiple lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A\n",
    "\n",
    "Extract about 5-6k reviews. However, many reviews may not have any text and will therefore be discarded. Finally you may end up with 1700-2000 reviews with text.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        messages = soup.find_all(\"div\", class_ = \"Message userContent\")\n",
    "\n",
    "        dates = soup.find_all(\"time\")\n",
    "\n",
    "        data = []\n",
    "\n",
    "        for message, date in zip(messages, dates):\n",
    "            message_text = message.get_text(strip = True)\n",
    "            date_text = date.get(\"title\")\n",
    "            data.append({\"Date\": date_text, \"Message\": message_text})\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "def scrape_forum(base_url, total_pages):\n",
    "    all_data = []\n",
    "\n",
    "    for page_num in range(1, total_pages + 1):\n",
    "        page_url = f\"{base_url}/p{page_num}\"\n",
    "        print(f\"Scraping page {page_num}: {page_url}\")\n",
    "        page_data = scrape_page(page_url)\n",
    "        all_data.extend(page_data)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.beeradvocate.com/beer/top-rated/\"\n",
    "total_pages = 300\n",
    "forum_data = scrape_forum(base_url, total_pages)\n",
    "messagedata = pd.DataFrame(forum_data)\n",
    "messagedata.to_csv(\"messagedata.csv\", index = False)\n",
    "len(messagedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B\n",
    "\n",
    "Assume that a customer, who will be using this recommender system, has specified 3 attributes in a product. E.g., one website describes multiple attributes of beer (but you should choose attributes from the actual data like you did for the first assignment)\n",
    "\n",
    "https://www.dummies.com/food-drink/drinks/beer/beer-for-dummies-cheat-sheet/\n",
    "- Aggressive (Boldly assertive aroma and/or taste) \n",
    "- Balanced: Malt and hops in similar proportions; equal representation of malt sweetness and hop bitterness in the flavor — especially at the finish\n",
    "- Complex: Multidimensional; many flavors and sensations on the palate\n",
    "- Crisp: Highly carbonated; effervescent\n",
    "- Fruity: Flavors reminiscent of various fruits or Hoppy: Herbal, earthy, spicy, or citric aromas and flavors of hops or Malty: Grainy, caramel-like; can be sweet or dry\n",
    "- Robust: Rich and full-bodied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports & load data\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data = pd.read_csv('beer_comments_final.csv')\n",
    "df = data\n",
    "\n",
    "#clean message col; make lowercase, remove special chars + numbers\n",
    "df['cleaned_comment'] = df['translated_comment'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x.lower()) if isinstance(x, str) else '')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word freq analysis\n",
    "#countVectorizer to get word freq counts\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "#fit + transform cleaned messages to get term-document matrix\n",
    "X = vectorizer.fit_transform(df['cleaned_comment'])\n",
    "\n",
    "#sum of counts for each word\n",
    "word_counts = X.sum(axis=0).tolist()[0]\n",
    "word_freq = dict(zip(vectorizer.get_feature_names_out(), word_counts))\n",
    "\n",
    "#top 10\n",
    "top_words = Counter(word_freq).most_common(10)\n",
    "print('Top 10 Words by Frequency:')\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select top 5 most freq mentioned words as attributes\n",
    "top_attributes = [word for word, count in Counter(word_freq).most_common(5)]\n",
    "print('Top 5 Attributes:')\n",
    "top_attributes #obviously we'll have to look through the real file to get true attributes instead of random top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lift analysis\n",
    "#get cleaned messages + attributes from word freq\n",
    "input_file = df['cleaned_comment'].apply(lambda x: x.split() if isinstance(x, str) else [])\n",
    "all_items = top_attributes\n",
    "\n",
    "#init counters\n",
    "total_posts = len(input_file)\n",
    "attribute_counts = {item: 0 for item in all_items}\n",
    "co_occurrences = {pair: 0 for pair in combinations(all_items, 2)}\n",
    "\n",
    "#count occurrences + co-occurrences\n",
    "for tokens in input_file:\n",
    "    if isinstance(tokens, list):  #checks that tokens are in a list\n",
    "        #individual occurrences\n",
    "        unique_tokens = set(tokens)  #avoid double counting in same post\n",
    "        for token in unique_tokens:\n",
    "            if token in attribute_counts:\n",
    "                attribute_counts[token] += 1\n",
    "                \n",
    "        #co-occurrences for attribute pairs\n",
    "        for attr_a, attr_b in combinations(top_attributes, 2):\n",
    "            if attr_a in unique_tokens and attr_b in unique_tokens:\n",
    "                co_occurrences[(attr_a, attr_b)] += 1\n",
    "\n",
    "#init lift matrix df\n",
    "lift_matrix = pd.DataFrame(0.0, index=all_items, columns=all_items)\n",
    "\n",
    "#calc lift for each pair\n",
    "for pair, co_count in co_occurrences.items():\n",
    "    attr_a, attr_b = pair\n",
    "    p_a = attribute_counts[attr_a] / total_posts\n",
    "    p_b = attribute_counts[attr_b] / total_posts\n",
    "    p_a_and_b = co_count / total_posts\n",
    "\n",
    "    if p_a > 0 and p_b > 0: #avoid division by zero\n",
    "        lift = p_a_and_b / (p_a * p_b)\n",
    "        if lift_matrix.index.get_loc(attr_a) < lift_matrix.index.get_loc(attr_b):\n",
    "            lift_matrix.at[attr_a, attr_b] = lift\n",
    "\n",
    "lift_matrix = lift_matrix.fillna(0.0)\n",
    "\n",
    "#actual lift matrix\n",
    "print('Lift Matrix:')\n",
    "lift_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case you wanted to see the lift values dict\n",
    "{pair: (co_occurrences[pair] / total_posts) / ((attribute_counts[pair[0]] / total_posts) * (attribute_counts[pair[1]] / total_posts)) for pair in co_occurrences}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task C\n",
    "\n",
    "Perform a similarity analysis using cosine similarity (without word embeddings – i.e., using the bag-of-words model) with the 3 attributes specified by the customer and the reviews. \n",
    "The similarity script should accept as input a file with the product attributes, and calculate similarity scores (between 0 and 1) between these attributes and each review. That is, the output file should have 3 columns – product_name (for each product, the product_name will repeat as many times as there are reviews of the product), product_review and similarity_score. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task D\n",
    "\n",
    "For every review, perform a sentiment analysis (using VADER or any LLM). In case you have to change the default values of words in the VADER lexicon, use this article: https://medium.com/swlh/adding-context-to-unsupervised-sentiment-analysis-7b6693d2c9f8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task E\n",
    "\n",
    "Create an evaluation score for each beer that uses both similarity and sentiment scores. \n",
    "Now recommend 3 products to the customer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task F \n",
    "\n",
    "How would your recommendations change if you use word vectors (e.g., the spaCy package with medium sized pretrained word vectors) instead of the plain vanilla bag-of-words cosine similarity? One way to analyze the difference would be to consider the % of reviews that mention a preferred attribute. E.g., if you recommend a product, what % of its reviews mention an attribute specified by the customer? Do you see any difference across bag-of-words and word vector approaches? Explain. This article may be useful: https://medium.com/swlh/word-embeddings-versus-bag-of-words-the-curious-case-of-recommender-systems-6ac1604d4424?source=friends_link&sk=d746da9f094d1222a35519387afc6338\n",
    "\n",
    "\n",
    "Note that the article doesn’t claim that bag-of-words will always be better than word embeddings for recommender systems. It lays out conditions under which it is likely to be the case. That is, depending on the attributes you use, you may or may not see the same effect. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task G\n",
    "\n",
    "How would your recommendations differ if you ignored the similarity and feature sentiment scores and simply chose the 3 highest rated products from your entire dataset? Would these products meet the requirements of the user looking for recommendations? Why or why not? Justify your answer with analysis. Use the similarity and sentiment scores as well as overall ratings to answer this question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task H\n",
    "\n",
    "Choose any 10 beers in your data. Now choose any one of them, and find the most similar beer (among the remaining 9). Explain your method and logic. https://medium.datadriveninvestor.com/who-is-your-competitor-in-the-era-of-the-long-tail-d0ac24fedde8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
